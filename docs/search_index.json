[
["index.html", "R for Pogressive Campaigns Chapter 1 This will be a preface", " R for Pogressive Campaigns Josiah Parry 2019-06-10 Chapter 1 This will be a preface "],
["calculating-ptg.html", "Chapter 2 calculating PTG", " Chapter 2 calculating PTG Goal: Demonstrate how to take raw canvassing data and calculate a PTG. This writing assumes that the reader has at least a basic understanding of R and the tidyverse—particularly dplyr and ggplot2. Overview: reading in data aggregating joining tables One of the most important metrics to a field program is the percent to goal (PTG). Field staff have a target number of pledge cards or doors to knock on. It is important for the data manger to provide a reliant and robust PTG reporting system to enable organizing directors to make informed and data driven decisions. In this exercise we will use three datasets (located in the data folder). canvassing_results.csv: canvassing results from January to March van_turf_lookup.csv: dataset containing the region codes for each van user goals.csv: dataset containing the weekly pledge card goal Steps: read in canvassing results read in turf code look up table create new week variable aggregate on the weekly level read in goal and calculate PTG library(tidyverse) library(lubridate) canvass &lt;- read_csv(&quot;data/canvassing_results.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## date = col_date(format = &quot;&quot;), ## vol_yes = col_double() ## ) canvass ## # A tibble: 6,671 x 3 ## van_id date vol_yes ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 2019-03-15 1 ## 2 2 2019-01-30 1 ## 3 3 2019-02-27 0 ## 4 4 2019-02-21 0 ## 5 5 2019-01-13 0 ## 6 6 2019-01-29 0 ## 7 7 2019-01-07 0 ## 8 8 2019-02-15 0 ## 9 9 2019-01-30 0 ## 10 10 2019-03-06 0 ## # … with 6,661 more rows I always briefly inspect my data using the count() function. Let’s count the number of individuals who marked volunteer yes. count(canvass, vol_yes) ## # A tibble: 2 x 2 ## vol_yes n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 4617 ## 2 1 2054 That is a lot of volunteers! They will go into the volunter recruitment and management pipeline and hopefully convert into some volunteer shifts. But which region are these potential volunteers in? To figure this out we will have to read in the van_turf_lookup.csv dataset. turf_lookup &lt;- read_csv(&quot;data/van_turf_lookup.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## turf_code = col_character() ## ) turf_lookup ## # A tibble: 6,004 x 2 ## van_id turf_code ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 B ## 2 2 E ## 3 3 E ## 4 4 B ## 5 5 C ## 6 6 E ## 7 7 B ## 8 8 C ## 9 9 D ## 10 10 A ## # … with 5,994 more rows Here we see that there are only two variables, van_id, and turf_code. This is a very common structure in relational data architectures. Because this table and the canvass table both share the van_id column we can merge the who based on this. This is referred to as a “common identifier”. The operation of joining two tables together is called a join. For more on joins and relational data please read chapter 13 of R for Data Science by Hadley Wickham. left_join(canvass, turf_lookup, by = &quot;van_id&quot;) ## # A tibble: 6,671 x 4 ## van_id date vol_yes turf_code ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2019-03-15 1 B ## 2 2 2019-01-30 1 E ## 3 3 2019-02-27 0 E ## 4 4 2019-02-21 0 B ## 5 5 2019-01-13 0 C ## 6 6 2019-01-29 0 E ## 7 7 2019-01-07 0 B ## 8 8 2019-02-15 0 C ## 9 9 2019-01-30 0 D ## 10 10 2019-03-06 0 A ## # … with 6,661 more rows This code node provides the turf codes for each van_id, but we still do not have the week that each observation belongs to.We are interested in the weekly pledge card goal so it is important to extract the calendar week from the date field. We will use the function lubridate::week() to do this. We will pipe the resultant table from the join into a mutate call where we will create this new variable and save it to an object called canvass_clean. canvass_clean &lt;- left_join(canvass, turf_lookup, by = &quot;van_id&quot;) %&gt;% mutate(week = week(date)) canvass_clean ## # A tibble: 6,671 x 5 ## van_id date vol_yes turf_code week ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2019-03-15 1 B 11 ## 2 2 2019-01-30 1 E 5 ## 3 3 2019-02-27 0 E 9 ## 4 4 2019-02-21 0 B 8 ## 5 5 2019-01-13 0 C 2 ## 6 6 2019-01-29 0 E 5 ## 7 7 2019-01-07 0 B 1 ## 8 8 2019-02-15 0 C 7 ## 9 9 2019-01-30 0 D 5 ## 10 10 2019-03-06 0 A 10 ## # … with 6,661 more rows We can use count() again to explore the pledge cards by region and week. We can add unquoted column names as arguments to count() which will be used to group the data. count(canvass_clean, turf_code) ## # A tibble: 6 x 2 ## turf_code n ## &lt;chr&gt; &lt;int&gt; ## 1 A 1137 ## 2 B 1101 ## 3 C 1159 ## 4 D 1114 ## 5 E 1044 ## 6 F 1116 count(canvass_clean, week) ## # A tibble: 11 x 2 ## week n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 651 ## 2 2 629 ## 3 3 552 ## 4 4 637 ## 5 5 607 ## 6 6 614 ## 7 7 617 ## 8 8 643 ## 9 9 641 ## 10 10 640 ## 11 11 440 count(canvass_clean, turf_code, week) ## # A tibble: 66 x 3 ## turf_code week n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 1 114 ## 2 A 2 109 ## 3 A 3 96 ## 4 A 4 121 ## 5 A 5 95 ## 6 A 6 99 ## 7 A 7 78 ## 8 A 8 111 ## 9 A 9 119 ## 10 A 10 119 ## # … with 56 more rows Though these counts (you may be more familiar with the phrase cross-tabs) are extremely useful, we still want to know the number of volunteers pledged. For more control over the aggregate measures, we will use dplyr::group_by() and dplyr::summarise() (for more see chapter 5.6 in R for Data Science). We will create a new table called weekly_canvass which is grouped by turf code and week. This table will have a column for turf_code, week, the number of people pledged to vote n_pledged, and the number of people who indicated they would volunteer vol_yes. weekly_canvass &lt;- canvass_clean %&gt;% group_by(turf_code, week) %&gt;% summarise(n_pledged = n(), vol_yes = sum(vol_yes)) weekly_canvass ## # A tibble: 66 x 4 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 1 114 41 ## 2 A 2 109 28 ## 3 A 3 96 33 ## 4 A 4 121 34 ## 5 A 5 95 32 ## 6 A 6 99 29 ## 7 A 7 78 28 ## 8 A 8 111 29 ## 9 A 9 119 34 ## 10 A 10 119 27 ## # … with 56 more rows Now that we have our counts of pledges and volunteers by week and turf code we need to compare this to their weekly goal. The weekly goals are in goals.csv. goals &lt;- read_csv(&quot;data/goals.csv&quot;) ## Parsed with column specification: ## cols( ## week = col_double(), ## region = col_character(), ## goal = col_double() ## ) goals ## # A tibble: 312 x 3 ## week region goal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A 120 ## 2 1 B 130 ## 3 1 C 70 ## 4 1 D 120 ## 5 1 E 90 ## 6 1 F 130 ## 7 2 A 110 ## 8 2 B 90 ## 9 2 C 130 ## 10 2 D 70 ## # … with 302 more rows Again, this data will need to be joined. What is unique here though is that there is not a single common identifier column. We will need to join on two columns. Namely, region (turf code), and week. Notice that we have mismatched names. To perform a join in this scenario we will need to provide a named vector to the by argument (more on named vectors in chapter 20.4.4 in R for Data Science). The name of the vector element is the column name in the left hand table and the value is the name of the column in the right hand table. In our case, the left hand table is weekly_canvass which has the column name turf_code. The right hand table is goals which has the column name region. To match on this we have to provide the named vector c(&quot;turf_code&quot; = &quot;region). Since the second column we are matching on is week which is present in both tables, this element does not have to be named. Thus the vector we will use is c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;). left_join(weekly_canvass, goals, by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) ## # A tibble: 66 x 5 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes goal ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 120 ## 2 A 2 109 28 110 ## 3 A 3 96 33 60 ## 4 A 4 121 34 60 ## 5 A 5 95 32 90 ## 6 A 6 99 29 120 ## 7 A 7 78 28 110 ## 8 A 8 111 29 130 ## 9 A 9 119 34 130 ## 10 A 10 119 27 130 ## # … with 56 more rows With this join we see that we have the goal and the actual number pledged. We’re one step away from calculating the PTG! To calculate the percent we need to divide the actual number by the goal and multiply by 100. We will do this within a mutate call after we join and save this to a new object ptg. ptg &lt;- left_join(weekly_canvass, goals, by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) %&gt;% mutate(ptg = (n_pledged / goal) * 100) ptg ## # A tibble: 66 x 6 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 120 95 ## 2 A 2 109 28 110 99.1 ## 3 A 3 96 33 60 160 ## 4 A 4 121 34 60 202. ## 5 A 5 95 32 90 106. ## 6 A 6 99 29 120 82.5 ## 7 A 7 78 28 110 70.9 ## 8 A 8 111 29 130 85.4 ## 9 A 9 119 34 130 91.5 ## 10 A 10 119 27 130 91.5 ## # … with 56 more rows ptg %&gt;% ggplot(aes(week, ptg, color = turf_code)) + geom_point() + geom_line() + theme_minimal() + facet_wrap(~turf_code) + geom_hline(yintercept = 100, lty = 2) + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), legend.position = &quot;bottom&quot; ) + labs(title = &quot;PTG by Turf Code&quot;, x = &quot;Week&quot;, y = &quot;%&quot;) While having a table and chart for weekly PTG is extremely useful, it is important to provide a PTG metric for the entire program. To do this we will need to calculate an aggregate measure from the goals table and join this to an aggregated canvass table. We will start by aggregating the goals and creating a total_goals object. Note that in the code chunk below I rename the region column in the group_by() statement this will be useful in the future so we can avoid having to use a named vector in our join. total_goals &lt;- goals %&gt;% group_by(turf_code = region) %&gt;% summarise(goal = sum(goal)) total_goals ## # A tibble: 6 x 2 ## turf_code goal ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4390 ## 2 B 4530 ## 3 C 4470 ## 4 D 4350 ## 5 E 4640 ## 6 F 4720 Now that we have total_goals we need to know the total number of pledges each region has gathered. We will use the existing canvass_clean object and count the total number or pledges using count(). total_pledges &lt;- count(canvass_clean, turf_code) total_pledges ## # A tibble: 6 x 2 ## turf_code n ## &lt;chr&gt; &lt;int&gt; ## 1 A 1137 ## 2 B 1101 ## 3 C 1159 ## 4 D 1114 ## 5 E 1044 ## 6 F 1116 Now we can join these two tables together and calculate a program wide PTG. total_ptg &lt;- inner_join(total_pledges, total_goals, by = &quot;turf_code&quot;) %&gt;% mutate(ptg = n / goal * 100) total_ptg ## # A tibble: 6 x 4 ## turf_code n goal ptg ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1137 4390 25.9 ## 2 B 1101 4530 24.3 ## 3 C 1159 4470 25.9 ## 4 D 1114 4350 25.6 ## 5 E 1044 4640 22.5 ## 6 F 1116 4720 23.6 Everyone loves a bar chart to visually understand their data. total_ptg %&gt;% mutate(turf_code = fct_rev(turf_code)) %&gt;% ggplot(aes(turf_code, ptg)) + geom_col() + geom_hline(yintercept = 100, lty = 2) + coord_flip() + labs(title = &quot;Cumulative PTG&quot;) + theme_minimal() + theme( panel.grid.major.x = element_blank() ) To Do: Turn into a report turn into parameterized report emailing with gmailr hosting and scheduling with connect "],
["ptg-sql.html", "Chapter 3 ptg-sql", " Chapter 3 ptg-sql create sqllite db for the existing sample data go over creating the appropriate sql code. "],
["reporting-googlesheets.html", "Chapter 4 reporting-googlesheets", " Chapter 4 reporting-googlesheets Goal: Create a daily percent to goal report As described previously, a common practice is to export aggregated code from Civis into a Google Sheet. In this section we will work with Google Sheets to create a report that can be used to schedule automatic reporting. To use the googlesheets package you must first install it (install.packages(&quot;googlesheets&quot;)). To load a spreadsheet use the gs_title() or gs_url() functions. The former takes the name of the sheet and the latter uses the url of it. Once this line is ran Google will open and require you to authenticate. This will create .httr-oath file in your working directory. This contains your authorization token which will be used later for automating this workflow. library(googlesheets) # register the sheet sheet &lt;- gs_title(&quot;R 4 Progressive Campaigns&quot;) ## Sheet successfully identified: &quot;R 4 Progressive Campaigns&quot; # read the `weekly_canvas` tab weekly_canvass &lt;- gs_read(sheet, &quot;weekly_canvass&quot;) ## Accessing worksheet titled &#39;weekly_canvass&#39;. ## Parsed with column specification: ## cols( ## turf_code = col_character(), ## week = col_double(), ## n_pledged = col_double(), ## vol_yes = col_double() ## ) # read the `goals` tab goals &lt;- gs_read(sheet, &quot;goals&quot;) ## Accessing worksheet titled &#39;goals&#39;. ## Parsed with column specification: ## cols( ## week = col_double(), ## region = col_character(), ## goal = col_double() ## ) weekly_canvass ## # A tibble: 66 x 4 ## turf_code week n_pledged vol_yes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 ## 2 A 2 109 28 ## 3 A 3 96 33 ## 4 A 4 121 34 ## 5 A 5 95 32 ## 6 A 6 99 29 ## 7 A 7 78 28 ## 8 A 8 111 29 ## 9 A 9 119 34 ## 10 A 10 119 27 ## # … with 56 more rows goals ## # A tibble: 312 x 3 ## week region goal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A 120 ## 2 1 B 130 ## 3 1 C 70 ## 4 1 D 120 ## 5 1 E 90 ## 6 1 F 130 ## 7 2 A 110 ## 8 2 B 90 ## 9 2 C 130 ## 10 2 D 70 ## # … with 302 more rows Notice that this code creates two tibbles in memory. Now everything else is the same. We will work within the context of an R Markdown document (intro to R Markdown). We want to create a simple report which will be emailed out to the organizing director each morning with the most up to date weekly PTG. Since this is a weekly report, we want to filter to the current week. In this case, the most recent data is from the 11th week of the month. We will filter both the weekly_canvass and goals tibbles to these weeks and then join. Note that it is important to filter before joining as joining can be computationally intensive. We want to reduce the data before joining it. current_week &lt;- weekly_canvass %&gt;% filter(week == 11) %&gt;% left_join(filter(goals, week == 11), by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) %&gt;% mutate(ptg = n_pledged / goal) current_week ## # A tibble: 6 x 6 ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 11 76 20 90 0.844 ## 2 B 11 51 20 50 1.02 ## 3 C 11 87 17 130 0.669 ## 4 D 11 83 31 60 1.38 ## 5 E 11 55 20 70 0.786 ## 6 F 11 88 29 70 1.26 While it nice to have each region’s PTG, it’s also useful to have the entire program weekly PTG. We can create an aggregate table and bind it to the existing table. totals &lt;- current_week %&gt;% bind_rows( current_week %&gt;% summarise(n_pledged = sum(n_pledged), goal = sum(goal), ptg = n_pledged / goal, vol_yes = sum(vol_yes), turf_code = &quot;Total&quot;, week = mean(week)) ) totals ## # A tibble: 7 x 6 ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 11 76 20 90 0.844 ## 2 B 11 51 20 50 1.02 ## 3 C 11 87 17 130 0.669 ## 4 D 11 83 31 60 1.38 ## 5 E 11 55 20 70 0.786 ## 6 F 11 88 29 70 1.26 ## 7 Total 11 440 137 470 0.936 totals %&gt;% ggplot(aes(turf_code, ptg)) + geom_col() + geom_hline(yintercept = 1, lty = 2, alpha = .4) + theme_minimal() + scale_y_continuous(labels = scales::percent_format()) + labs(title = &quot;Weekly PTG&quot;, y = &quot;% to goal&quot;, x = &quot;Turf Code&quot;) It is important that a clean table is presented alongside the chart. For this we will use knitr::kable(). totals %&gt;% mutate(ptg = ptg * 100) %&gt;% select(`Turf Code` = turf_code, `# Pledged` = n_pledged, Goal = goal, `% to Goal` = ptg) %&gt;% knitr::kable(digits = 2) Turf Code # Pledged Goal % to Goal A 76 90 84.44 B 51 50 102.00 C 87 130 66.92 D 83 60 138.33 E 55 70 78.57 F 88 70 125.71 Total 440 470 93.62 "],
["reporting.html", "Chapter 5 reporting", " Chapter 5 reporting creating reproducible reports common reports: daily emails for weekly ptg end of the week ptg weekly cumulative report parameterized r markdown hosting on connect "],
["mapping-doors-targets.html", "Chapter 6 mapping-doors-targets", " Chapter 6 mapping-doors-targets create choropleth of canvassing PTG by district "],
["shiny-creating-interactive-dashboards.html", "Chapter 7 shiny - creating interactive dashboards", " Chapter 7 shiny - creating interactive dashboards "],
["google-trends-results.html", "Chapter 8 google trends results 8.1 Google Trends Data 8.2 trendyy 8.3 Visualizing Trends", " Chapter 8 google trends results Over the past few years we have seen Google Trends becoming quite ubiquitous in politics. Pundits have used Google seach trends as talking points. It is not uncommon to hear news about a candidates search trends the days following a town hall or significant rally. It seems that Google trends are becoming the go to proxy for a candidate’s salience. As a campaign, you are interested in the popularity of a candidate relative to another one. If candidate A has seen a gain from 50 to 70, that is all well and good. But how does that compare with candidates C and D? There are others potential use cases—that may be less fraught with media interruptions. For example, one can keep track of the popularity of possible policy issues—i.e. healthcare, gun safety, women’s rights. Though the usefulness of Google Trends search popularity is still unclear, it may be something that your campaign might like to track. In this chapter we will explore how to acquire and utilize trend data using R. This chapter will describe how one can utilize Google Trends data to compare candidate search popularity and view related search terms. This will be done with the tidyverse, and the package trendyy for accessing this data. 8.1 Google Trends Data 8.1.1 Relative Popularity The key metric that Google Trends provides is the relative popularity of a search term by a given geography. Relative search popularity is scaled from 0 to 100. This number is scaled based on population and geography size (for more information go here). This number may be useful on it’s own, but the strength of Google Trends is it’s ability to compare multiple terms. Using Google Trends we can compare up to 5 search terms—presumably candidates. 8.1.2 Related Queries In addition to popularity, Google Trends provides you with related queries. This can help your media team understand in what context their candidate is being associated online. 8.2 trendyy Now that we have an intuition of how Google Trends may be utilized, we will look at how actually acquire these data in R. To get started install the package using install.packages(&quot;trendyy&quot;). Once the package is installed, load the tidyverse and trendyy. library(trendyy) library(tidyverse) In this example we will look at the top five polling candidates as of today (6/10/2019). These are, in no particular order, Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren. Create a vector with the search terms that you will use (in this case the above candidates). candidates &lt;- c(&quot;Joe Biden&quot;, &quot;Kamala Harris&quot;, &quot;Beto O&#39;Rourke&quot;, &quot;Bernie Sanders&quot;, &quot;Elizabeth Warren&quot;) Next we will use the trendyy package to get search popularity. The function trendy() has three main arguments: search_terms, from, and to (in the form of &quot;yyyy-mm-dd&quot;). The first argument is the only mandatory one. Provide a vector of length 5 or less as the first argument. Here we will use the candidates vector and look at data from the past two weeks. I will create two variables for the beginning and end dates. This will be to demonstrate how functions can be used to programatically search date ranges. # to today end &lt;- Sys.Date() # from 2 weeks ago begin &lt;- Sys.Date() - 14 Pass these arguments to trendy() and save them to a variable. candidate_trends &lt;- trendy(search_terms = candidates, from = begin, to = end) candidate_trends ## ~Trendy results~ ## ## Search Terms: Joe Biden, Kamala Harris, Beto O&#39;Rourke, Bernie Sanders, Elizabeth Warren ## ## (&gt;^.^)&gt; ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ &lt;(^.^&lt;) ## # A tibble: 5 x 5 ## keyword max_hits min_hits from to ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 Bernie Sanders 41 32 2019-05-27 2019-06-07 ## 2 Beto O&#39;Rourke 2 1 2019-05-27 2019-06-07 ## 3 Elizabeth Warren 69 16 2019-05-27 2019-06-07 ## 4 Joe Biden 72 35 2019-05-27 2019-06-07 ## 5 Kamala Harris 100 13 2019-05-27 2019-06-07 Trendy creates an object of class trendy see class(candidate_trends) trendy. There are a number of accessor functions. We will use get_interest() and get_related_queries(). See the documentation of the others. To access to relative popularity, we will use get_interest(trendy). popularity &lt;- get_interest(candidate_trends) popularity ## # A tibble: 60 x 7 ## date hits geo time keyword gprop category ## &lt;dttm&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-05-27 00:00:00 47 world 2019-05-27 2… Joe Bid… web All catego… ## 2 2019-05-28 00:00:00 61 world 2019-05-27 2… Joe Bid… web All catego… ## 3 2019-05-29 00:00:00 72 world 2019-05-27 2… Joe Bid… web All catego… ## 4 2019-05-30 00:00:00 59 world 2019-05-27 2… Joe Bid… web All catego… ## 5 2019-05-31 00:00:00 41 world 2019-05-27 2… Joe Bid… web All catego… ## 6 2019-06-01 00:00:00 36 world 2019-05-27 2… Joe Bid… web All catego… ## 7 2019-06-02 00:00:00 35 world 2019-05-27 2… Joe Bid… web All catego… ## 8 2019-06-03 00:00:00 38 world 2019-05-27 2… Joe Bid… web All catego… ## 9 2019-06-04 00:00:00 44 world 2019-05-27 2… Joe Bid… web All catego… ## 10 2019-06-05 00:00:00 51 world 2019-05-27 2… Joe Bid… web All catego… ## # … with 50 more rows For related queries we will use get_related_queries(trendy). Note that you can either pipe the object or pass it directly. candidate_trends %&gt;% get_related_queries() %&gt;% # picking queries for a random candidate filter(keyword == sample(candidates, 1)) ## # A tibble: 36 x 5 ## subject related_queries value keyword category ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 100 top trump Joe Biden All categories ## 2 43 top joe biden 2020 Joe Biden All categories ## 3 39 top joe biden age Joe Biden All categories ## 4 38 top bernie sanders Joe Biden All categories ## 5 38 top donald trump Joe Biden All categories ## 6 23 top joe biden news Joe Biden All categories ## 7 18 top how old is joe biden Joe Biden All categories ## 8 17 top joe biden campaign Joe Biden All categories ## 9 17 top creepy joe biden Joe Biden All categories ## 10 17 top joe biden twitter Joe Biden All categories ## # … with 26 more rows 8.3 Visualizing Trends I’m guessing your director enjoys charts—so do I. To make the data more accessible, use the popularity tibble to create a time series chart of popularity over the past two weeks. We will use ggplot2. Remember that time should be displayed on the x axis. We want to have a line for each candidate, so map the color aesthetic to the keyword. ggplot(popularity, aes(x = date, y = hits, color = keyword)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Search Popularity&quot;, title = &quot;Google popularity of top 5 polling candidates&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) "],
["workflow.html", "Chapter 9 workflow", " Chapter 9 workflow This will be an overview of common data infrastructure in progressive campaigns. We need to start with the golden rule of data “if it is not in VAN, it does not exist”. What is VAN? Voter Activation Network It is not the VAN it is VAN. go over types of voter outreach VAN is the location where all voter interactions live. VAN is our database for voter outreach. When contact is made with a potential voter that is entered in VAN. There are many common ways that this data is recorded. Crowd canvassing—collecting pledge cards, petitioon sign ups. Door-to-door canvassing—knocking on doors and having direct conversations with voters. This is sometimes referred to “knocking doors”. Two other common types of voter outreach is phone banking and text banking. Data is collected and entered into VAN - pledge cards are collected, then entered into VAN (the same night) What is Civis? Civis analytis is a company that creates a data sciecne platforms which is used by many progressive campaigns. They provide secure cloud based solution for data warehousing, analysis, and workflow automation, among others. It is common for many campaigns to have daily exports from VAN into Civis. This provides data directors and data managers the ability to programmatically analyse VAN data. Since data are avaialble in a database, they can be queried using SQL. SQL scripts are written to create aaggregated data. A lot of the power of this comes from the ability to create workflows in Civis. The output of SQL scripts can be exported to a Google Sheet which is much more accessible for the less technically inclined. This also is great for creating shareable sheets-based reporting. The workflow: This guide/book will try to replicate this workflow as best as possible. "],
["web-scraping-polling-use-case.html", "Chapter 10 Web-Scraping: Polling use case 10.1 Understanding rvest", " Chapter 10 Web-Scraping: Polling use case A very important metric to keep track of is how your candidate is polling. Are they gaining a lead in the polls or falling behind? This data is often reported via traditional news organizations or some other mediums. The supposed demi-God and mythical pollster Nate Silver’s organization FiveThirtyEight does a wonderful job aggregating polls. Their page National 2020 Democratic Presidential Primary Polls has a table of the most recent polls from many different pollsters. In this use case we will scrape acquire this data through webscraping using rvest. We will also go over ways to programatically save polls results to a text file. Saving polling results can allow you present a long term view of your candidate’s growth during the quarter. 10.1 Understanding rvest This use case will provide a cursory overview of the package rvest. To learn more go here. Web scraping is the process of extracting data from a website. Websites are written in HTML and CSS. There are a few aspects of these languages that are used in webscraping that is important to know. HTML is written in a series of what are call tags. A tag is a set of characters wrapped in angle brackets—i.e. &lt;img&gt;. With CSS (cascading style sheets), web developers can give unique identifiers to a tag. Classes can also be assigned to a tag. Think of these as group. With web scraping we can specify a particular part of a website by it’s HTML tag and perhaps it’s class or ID. rvest provides a large set of functions to make this simpler. "],
["example.html", "Chapter 11 Example", " Chapter 11 Example For this example we will be scraping FiveThirtyEight’s aggregated poll table. The table can be found at https://projects.fivethirtyeight.com/2020-primaries/democratic/national/. Before we begin, we must always prepare our workspace. Mise en place. library(rvest) library(tidyverse) The first thing we will have to do is specify what page we will be scraping from. html_session() will simulate a session in an html browser. By providing a url to html_session() we will then be able to access the underlying code of that page. Create an object called session by providing the FiveThirtyEight url to html_session(). session &lt;- html_session(&quot;https://projects.fivethirtyeight.com/2020-primaries/democratic/national/&quot;) The next and most important step is to identify which piece of HTML code contains the table. The easiest way to do this is to open up the webpage in Chrome and open up the Inspect Elements view (on Mac - ⌘ + Shift + C). Now that this is open, click the select element button at the top left corner of the inspection pane. Now hover over the table. You will see that the HTML element is highlighted. We can see that it is a table tag. Additionally we see that there are two different classes polls-table and tracker. To specify a class we put a preceeding . to the class name—i.e. .class-name. If there are multiple classess we just append the second class named to it—i.e. .first-class.second-class. Be aware that these selectors can be quite finicky and be a bit difficult to figure out. You might need to do some googling or playing around with the selector. To actually access the content of this HTML element, we must specify the element using the proper selector. html_node() will be used to do this. Provide the html session and the css selector to html_node() to extract the HTML element. session %&gt;% html_node(&quot;.polls-table.tracker&quot;) ## {xml_node} ## &lt;table class=&quot;polls-table tracker&quot;&gt; ## [1] &lt;thead class=&quot;hide-mobile&quot; id=&quot;table-header&quot;&gt;&lt;tr&gt;\\n&lt;th class=&quot;new&quot;&gt;&lt; ... ## [2] &lt;tbody&gt;\\n&lt;tr class=&quot;visible-row&quot; data-id=&quot;97589&quot;&gt;\\n&lt;!-- Shared--&gt;&lt;td ... Here we see that this returns on object of class xml_node. This object returns some HTML code but it is still not entirely workable. Since this is an HTML table we want to extract we can use the handy html_table(). Note that if this wasn’t a table but rather text, you can use html_text(). session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table() Take note of the extremely informative error. It appears we might have to deal with mismatching columns. session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table(fill = TRUE) ## Dates Pollster Sample ## 1 • May 29-Jun 5, 20192,271 RV May 29-Jun 5, 2019 B+Ipsos ## 2 • May 29-Jun 5, 20192,525 A May 29-Jun 5, 2019 B+Ipsos ## 3 • Jun 2-4, 2019500 RV Jun 2-4, 2019 BYouGov ## 4 • Jun 2-4, 2019550 A Jun 2-4, 2019 BYouGov ## 5 • Jun 1-2, 2019431 RV Jun 1-2, 2019 C+HarrisX ## 6 • May 27-Jun 2, 201916,587 LV May 27-Jun 2, 2019 B-Morning Consult ## 7 • May 28-31, 2019412 RV May 28-31, 2019 A-CNN/SSRS ## 8 • May 29-30, 2019471 RV May 29-30, 2019 C+Harris Interactive ## 9 • May 28-30, 2019881 RV May 28-30, 2019 C+HarrisX ## 10 • May 28-30, 2019881 RV May 28-30, 2019 C+HarrisX ## 11 • May 28-30, 2019881 RV May 28-30, 2019 C+HarrisX ## 12 • May 28-30, 2019881 RV May 28-30, 2019 C+HarrisX ## 13 • May 28-30, 2019881 RV May 28-30, 2019 C+HarrisX ## 14 • May 20-26, 201916,368 LV May 20-26, 2019 B-Morning Consult ## 15 • May 23-25, 2019881 RV May 23-25, 2019 C+HarrisX ## 16 • May 20-21, 2019447 RV May 20-21, 2019 Echelon Insights ## Sample Biden Sanders Harris Warren O&#39;Rourke Buttigieg Booker Klobuchar ## 1 2,271 RV 31% 14% 6% 9% 3% 5% 2% ## 2 2,525 A 30% 15% 6% 8% 4% 5% 2% ## 3 500 RV 27% 15% 9% 12% 2% 10% 2% ## 4 550 A 27% 16% 8% 11% 2% 9% 2% ## 5 431 RV 35% 16% 4% 5% 4% 8% 3% ## 6 16,587 LV 38% 19% 7% 10% 4% 7% 3% ## 7 412 RV 32% 18% 8% 7% 5% 5% 3% ## 8 471 RV 36% 17% 8% 5% 4% 5% 3% ## 9 881 RV 42% 37% ## 10 881 RV 41% 38% ## 11 881 RV 43% 41% ## 12 881 RV 37% 40% ## 13 881 RV 39% 41% ## 14 16,368 LV 38% 20% 7% 9% 4% 7% 3% ## 15 881 RV 33% 15% 6% 7% 3% 5% 3% ## 16 447 RV 66% 19% ## Castro Yang Gillibrand Hickenlooper Gabbard Delaney Inslee Ryan Bullock ## 1 2% 1% 1% 0% 1% 1% 0% 0% 1% ## 2 1% 0% 1% 0% 1% 1% 0% 0% 1% ## 3 1% 0% 1% 0% 1% 1% 1% 0% 0% ## 4 1% 0% 1% 0% 1% 1% 1% 0% 0% ## 5 1% 0% 0% 0% 1% 0% 0% 0% 0% ## 6 1% 1% 1% 1% 1% 1% 1% 1% 1% ## 7 2% 2% 1% 1% 0% 1% 0% 1% 1% ## 8 0% 1% 1% 0% 1% 0% 1% 1% 1% ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 1% 1% 1% 1% 1% 1% 1% 0% 1% ## 15 1% 1% 1% 1% 1% 0% 1% 1% 1% ## 16 ## de Blasio Bennet Williamson Gravel Swalwell Moulton Messam H. Clinton ## 1 0% 1% 0% 0% 0% 0% 0% 0% ## 2 0% 1% 0% 0% 0% 0% 0% 0% ## 3 1% 2% 0% 0% 0% 0% 0% 0% ## 4 1% 2% 0% 0% 0% 0% 0% 0% ## 5 0% 0% 1% 1% 0% 0% 0% 0% ## 6 1% 1% 1% 1% 0% 0% ## 7 0% 0% 1% 0% 0% 0% 0% ## 8 0% 1% 0% 0% 0% ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 1% 0% 1% 1% 0% 0% ## 15 0% 0% 0% 0% 0% ## 16 ## Bloomberg M. Obama Brown Kerry Abrams Holder McAuliffe Winfrey Ojeda ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 1% ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## Trump Cuomo Avenatti Kennedy Patrick Zuckerberg Pelosi Garcetti Newsom ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## Steyer Schultz Kaine Johnson Kucinich Lee Scott Sinema Warner NA ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## NA ## 1 Biden31%Sanders14%Warren9%Harris6%Buttigieg5%O&#39;Rourke3%Booker2%Klobuchar2%Castro1%Gabbard1%Hickenlooper1%Yang1%Ryan1%de Blasio1%Gillibrand0%Bullock0%Inslee0%Delaney0%Williamson0%Messam0%Swalwell0%Moulton0%Bennet0%Gravel0% ## 2 Biden30%Sanders15%Warren8%Harris6%Buttigieg5%O&#39;Rourke4%Booker2%Klobuchar1%Gabbard1%Hickenlooper1%Yang1%Ryan1%de Blasio1%Castro0%Gillibrand0%Bullock0%Inslee0%Delaney0%Williamson0%Messam0%Swalwell0%Moulton0%Bennet0%Gravel0% ## 3 Biden27%Sanders15%Warren12%Buttigieg10%Harris9%Booker2%de Blasio2%O&#39;Rourke2%Bullock1%Delaney1%Gabbard1%Hickenlooper1%Klobuchar1%Yang1%Bennet0%Castro0%Gillibrand0%Gravel0%Inslee0%Messam0%Moulton0%Ryan0%Swalwell0%Williamson0% ## 4 Biden27%Sanders16%Warren11%Buttigieg9%Harris8%Booker2%de Blasio2%O&#39;Rourke2%Bullock1%Delaney1%Gabbard1%Hickenlooper1%Klobuchar1%Yang1%Bennet0%Castro0%Gillibrand0%Gravel0%Inslee0%Messam0%Moulton0%Ryan0%Swalwell0%Williamson0% ## 5 Biden35%Sanders16%Buttigieg8%Warren5%Harris4%O&#39;Rourke4%Booker3%Klobuchar1%Hickenlooper1%Williamson1%Bennet1%Yang0%Ryan0%Bullock0%Castro0%de Blasio0%Delaney0%Gabbard0%Gillibrand0%Inslee0%Swalwell0%Gravel0%Moulton0%Messam0% ## 6 Biden38%Sanders19%Warren10%Harris7%Buttigieg7%O&#39;Rourke4%Booker3%Bennet1%Bullock1%Castro1%de Blasio1%Delaney1%Gabbard1%Gillibrand1%Hickenlooper1%Inslee1%Klobuchar1%Ryan1%Yang1%Williamson1%Moulton0%Swalwell0% ## 7 Biden32%Sanders18%Harris8%Warren7%Buttigieg5%O&#39;Rourke5%Booker3%Castro2%Klobuchar2%Bennet1%Gabbard1%Gillibrand1%Inslee1%Ryan1%Yang1%de Blasio0%Delaney0%Hickenlooper0%Williamson0%Bullock0%Messam0%Moulton0%Swalwell0% ## 8 Biden36%Sanders17%Harris8%Warren5%Buttigieg5%O&#39;Rourke4%Booker3%Hickenlooper1%Gravel1%Ryan1%Yang1%Castro1%Bloomberg1%Inslee1%Delaney1%Swalwell0%Gillibrand0%Moulton0%Gabbard0%Klobuchar0%Williamson0%Messam0% ## 9 Sanders42%Harris37% ## 10 Biden41%Harris38% ## 11 Biden43%Sanders41% ## 12 Warren40%Sanders37% ## 13 Warren41%Biden39% ## 14 Biden38%Sanders20%Warren9%Buttigieg7%Harris7%O&#39;Rourke4%Booker3%Bennet1%Castro1%Delaney1%Gillibrand1%Hickenlooper1%Klobuchar1%Yang1%Williamson1%Bullock1%Ryan1%Gabbard1%de Blasio0%Inslee0%Moulton0%Swalwell0% ## 15 Biden33%Sanders15%Warren7%Harris6%Buttigieg5%Booker3%O&#39;Rourke3%Castro1%Delaney1%Gillibrand1%Hickenlooper1%Inslee1%Klobuchar1%Yang1%Ryan1%Gabbard0%Williamson0%Gravel0%Messam0%Moulton0%Swalwell0% ## 16 Biden66%Warren19% ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 98 rows ] This is much better! But based on visual insepction the column headers are not properly matched. There are a few things that need to be sorted out: there are two date columns, there are commas and percents where numeric columns should be, the column headers are a little messy, and the table isn’t a tibble (this is just personal preference). We will handle the final two issues first as they are easiest to deal with. The function clean_names() from janitor will handle the column headers, and as_tibble() will coerce the data.frame into a proper tibble. Save this semi-clean tibble into an object called polls. polls &lt;- session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table(fill = TRUE) %&gt;% janitor::clean_names() %&gt;% as_tibble() polls ## # A tibble: 114 x 59 ## x dates pollster sample sample_2 biden sanders harris warren ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 • May … May 29-… B+Ips… 2,271 RV 31% 14% 6% ## 2 • May … May 29-… B+Ips… 2,525 A 30% 15% 6% ## 3 • Jun … Jun 2-4… BYouG… 500 RV 27% 15% 9% ## 4 • Jun … Jun 2-4… BYouG… 550 A 27% 16% 8% ## 5 • Jun … Jun 1-2… C+Har… 431 RV 35% 16% 4% ## 6 • May … May 27-… B-Mor… 16,587 LV 38% 19% 7% ## 7 • May … May 28-… A-CNN… 412 RV 32% 18% 8% ## 8 • May … May 29-… C+Har… 471 RV 36% 17% 8% ## 9 • May … May 28-… C+Har… 881 RV &quot;&quot; 42% 37% ## 10 • May … May 28-… C+Har… 881 RV 41% &quot;&quot; 38% ## # … with 104 more rows, and 50 more variables: o_rourke &lt;chr&gt;, ## # buttigieg &lt;chr&gt;, booker &lt;chr&gt;, klobuchar &lt;chr&gt;, castro &lt;chr&gt;, ## # yang &lt;chr&gt;, gillibrand &lt;chr&gt;, hickenlooper &lt;chr&gt;, gabbard &lt;chr&gt;, ## # delaney &lt;chr&gt;, inslee &lt;chr&gt;, ryan &lt;chr&gt;, bullock &lt;chr&gt;, ## # de_blasio &lt;chr&gt;, bennet &lt;chr&gt;, williamson &lt;chr&gt;, gravel &lt;chr&gt;, ## # swalwell &lt;chr&gt;, moulton &lt;chr&gt;, messam &lt;chr&gt;, h_clinton &lt;chr&gt;, ## # bloomberg &lt;chr&gt;, m_obama &lt;chr&gt;, brown &lt;chr&gt;, kerry &lt;chr&gt;, ## # abrams &lt;chr&gt;, holder &lt;chr&gt;, mc_auliffe &lt;chr&gt;, winfrey &lt;chr&gt;, ## # ojeda &lt;chr&gt;, trump &lt;chr&gt;, cuomo &lt;chr&gt;, avenatti &lt;chr&gt;, kennedy &lt;chr&gt;, ## # patrick &lt;chr&gt;, zuckerberg &lt;chr&gt;, pelosi &lt;chr&gt;, garcetti &lt;chr&gt;, ## # newsom &lt;chr&gt;, steyer &lt;chr&gt;, schultz &lt;chr&gt;, kaine &lt;chr&gt;, johnson &lt;chr&gt;, ## # kucinich &lt;chr&gt;, lee &lt;chr&gt;, scott &lt;chr&gt;, sinema &lt;chr&gt;, warner &lt;chr&gt;, ## # na &lt;chr&gt;, na_2 &lt;chr&gt; We want to shift over the column names to the right just once. Unfortunately there is no elegant way to do this (that I am aware of). We can see that the first column is completely useless so that can be removed. Once that column is removed we can reset the names this way they will be well aligned. We will start by creating a vector of the original column names. col_names &lt;- names(polls) col_names ## [1] &quot;x&quot; &quot;dates&quot; &quot;pollster&quot; &quot;sample&quot; ## [5] &quot;sample_2&quot; &quot;biden&quot; &quot;sanders&quot; &quot;harris&quot; ## [9] &quot;warren&quot; &quot;o_rourke&quot; &quot;buttigieg&quot; &quot;booker&quot; ## [13] &quot;klobuchar&quot; &quot;castro&quot; &quot;yang&quot; &quot;gillibrand&quot; ## [17] &quot;hickenlooper&quot; &quot;gabbard&quot; &quot;delaney&quot; &quot;inslee&quot; ## [21] &quot;ryan&quot; &quot;bullock&quot; &quot;de_blasio&quot; &quot;bennet&quot; ## [25] &quot;williamson&quot; &quot;gravel&quot; &quot;swalwell&quot; &quot;moulton&quot; ## [29] &quot;messam&quot; &quot;h_clinton&quot; &quot;bloomberg&quot; &quot;m_obama&quot; ## [33] &quot;brown&quot; &quot;kerry&quot; &quot;abrams&quot; &quot;holder&quot; ## [37] &quot;mc_auliffe&quot; &quot;winfrey&quot; &quot;ojeda&quot; &quot;trump&quot; ## [41] &quot;cuomo&quot; &quot;avenatti&quot; &quot;kennedy&quot; &quot;patrick&quot; ## [45] &quot;zuckerberg&quot; &quot;pelosi&quot; &quot;garcetti&quot; &quot;newsom&quot; ## [49] &quot;steyer&quot; &quot;schultz&quot; &quot;kaine&quot; &quot;johnson&quot; ## [53] &quot;kucinich&quot; &quot;lee&quot; &quot;scott&quot; &quot;sinema&quot; ## [57] &quot;warner&quot; &quot;na&quot; &quot;na_2&quot; Unfortunately this also presents another issue. Once a column is deselected, there will be one more column name than column. So we will need to select all but the last element of the original names. We will create a vector called new_names. # identify the integer number of the last column last_col &lt;- length(col_names) - 1 # create a vector which will be used for the new names new_names &lt;- col_names[1:last_col] Now we can try implementing the hacky solution. Here we will deselect the first column and reset the names using setNames(). Following, we will use the mutate_at() variant to remove the percent sign from every candidate column and coerce them into integer columns. Here we will sepecify which variables to not mutate at within vars(). It’s impor polls %&gt;% select(-1) %&gt;% setNames(new_names)%&gt;% select(-1) %&gt;% mutate_at(vars(-c(&quot;dates&quot;, &quot;pollster&quot;, &quot;sample&quot;, &quot;sample_2&quot;)), ~as.integer(str_remove(., &quot;%&quot;))) ## Warning in (structure(function (..., .x = ..1, .y = ..2, . = ..1) : NAs ## introduced by coercion ## # A tibble: 114 x 57 ## dates pollster sample sample_2 biden sanders harris warren o_rourke ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 May … B+Ipsos 2,271 RV 31 14 6 9 3 ## 2 May … B+Ipsos 2,525 A 30 15 6 8 4 ## 3 Jun … BYouGov 500 RV 27 15 9 12 2 ## 4 Jun … BYouGov 550 A 27 16 8 11 2 ## 5 Jun … C+Harri… 431 RV 35 16 4 5 4 ## 6 May … B-Morni… 16,587 LV 38 19 7 10 4 ## 7 May … A-CNN/S… 412 RV 32 18 8 7 5 ## 8 May … C+Harri… 471 RV 36 17 8 5 4 ## 9 May … C+Harri… 881 RV NA 42 37 NA NA ## 10 May … C+Harri… 881 RV 41 NA 38 NA NA ## # … with 104 more rows, and 48 more variables: buttigieg &lt;int&gt;, ## # booker &lt;int&gt;, klobuchar &lt;int&gt;, castro &lt;int&gt;, yang &lt;int&gt;, ## # gillibrand &lt;int&gt;, hickenlooper &lt;int&gt;, gabbard &lt;int&gt;, delaney &lt;int&gt;, ## # inslee &lt;int&gt;, ryan &lt;int&gt;, bullock &lt;int&gt;, de_blasio &lt;int&gt;, ## # bennet &lt;int&gt;, williamson &lt;int&gt;, gravel &lt;int&gt;, swalwell &lt;int&gt;, ## # moulton &lt;int&gt;, messam &lt;int&gt;, h_clinton &lt;int&gt;, bloomberg &lt;int&gt;, ## # m_obama &lt;int&gt;, brown &lt;int&gt;, kerry &lt;int&gt;, abrams &lt;int&gt;, holder &lt;int&gt;, ## # mc_auliffe &lt;int&gt;, winfrey &lt;int&gt;, ojeda &lt;int&gt;, trump &lt;int&gt;, ## # cuomo &lt;int&gt;, avenatti &lt;int&gt;, kennedy &lt;int&gt;, patrick &lt;int&gt;, ## # zuckerberg &lt;int&gt;, pelosi &lt;int&gt;, garcetti &lt;int&gt;, newsom &lt;int&gt;, ## # steyer &lt;int&gt;, schultz &lt;int&gt;, kaine &lt;int&gt;, johnson &lt;int&gt;, ## # kucinich &lt;int&gt;, lee &lt;int&gt;, scott &lt;int&gt;, sinema &lt;int&gt;, warner &lt;int&gt;, ## # na &lt;int&gt; This is almost great to work with. We now need to remove the first date column, and remove the commas "]
]
