[
["index.html", "R for Pogressive Campaigns Chapter 1 This will be a preface", " R for Pogressive Campaigns Josiah Parry 2019-06-26 Chapter 1 This will be a preface "],
["calculating-ptg.html", "Chapter 2 calculating PTG", " Chapter 2 calculating PTG Goal: Demonstrate how to take raw canvassing data and calculate a PTG. This writing assumes that the reader has at least a basic understanding of R and the tidyverse—particularly dplyr and ggplot2. Overview: reading in data aggregating joining tables One of the most important metrics to a field program is the percent to goal (PTG). Field staff have a target number of pledge cards or doors to knock on. It is important for the data manger to provide a reliant and robust PTG reporting system to enable organizing directors to make informed and data driven decisions. In this exercise we will use three datasets (located in the data folder). canvassing_results.csv: canvassing results from January to March van_turf_lookup.csv: dataset containing the region codes for each van user goals.csv: dataset containing the weekly pledge card goal Steps: read in canvassing results read in turf code look up table create new week variable aggregate on the weekly level read in goal and calculate PTG library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date canvass &lt;- read_csv(&quot;data/canvassing_results.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## date = col_date(format = &quot;&quot;), ## vol_yes = col_double() ## ) canvass ## # A tibble: 6,671 x 3 ## van_id date vol_yes ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 2019-03-15 1 ## 2 2 2019-01-30 1 ## 3 3 2019-02-27 0 ## 4 4 2019-02-21 0 ## 5 5 2019-01-13 0 ## 6 6 2019-01-29 0 ## 7 7 2019-01-07 0 ## 8 8 2019-02-15 0 ## 9 9 2019-01-30 0 ## 10 10 2019-03-06 0 ## # … with 6,661 more rows I always briefly inspect my data using the count() function. Let’s count the number of individuals who marked volunteer yes. count(canvass, vol_yes) ## # A tibble: 2 x 2 ## vol_yes n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 4617 ## 2 1 2054 That is a lot of volunteers! They will go into the volunter recruitment and management pipeline and hopefully convert into some volunteer shifts. But which region are these potential volunteers in? To figure this out we will have to read in the van_turf_lookup.csv dataset. turf_lookup &lt;- read_csv(&quot;data/van_turf_lookup.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## turf_code = col_character() ## ) turf_lookup ## # A tibble: 6,004 x 2 ## van_id turf_code ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 B ## 2 2 E ## 3 3 E ## 4 4 B ## 5 5 C ## 6 6 E ## 7 7 B ## 8 8 C ## 9 9 D ## 10 10 A ## # … with 5,994 more rows Here we see that there are only two variables, van_id, and turf_code. This is a very common structure in relational data architectures. Because this table and the canvass table both share the van_id column we can merge the who based on this. This is referred to as a “common identifier”. The operation of joining two tables together is called a join. For more on joins and relational data please read chapter 13 of R for Data Science by Hadley Wickham. left_join(canvass, turf_lookup, by = &quot;van_id&quot;) ## # A tibble: 6,671 x 4 ## van_id date vol_yes turf_code ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2019-03-15 1 B ## 2 2 2019-01-30 1 E ## 3 3 2019-02-27 0 E ## 4 4 2019-02-21 0 B ## 5 5 2019-01-13 0 C ## 6 6 2019-01-29 0 E ## 7 7 2019-01-07 0 B ## 8 8 2019-02-15 0 C ## 9 9 2019-01-30 0 D ## 10 10 2019-03-06 0 A ## # … with 6,661 more rows This code node provides the turf codes for each van_id, but we still do not have the week that each observation belongs to.We are interested in the weekly pledge card goal so it is important to extract the calendar week from the date field. We will use the function lubridate::week() to do this. We will pipe the resultant table from the join into a mutate call where we will create this new variable and save it to an object called canvass_clean. canvass_clean &lt;- left_join(canvass, turf_lookup, by = &quot;van_id&quot;) %&gt;% mutate(week = week(date)) canvass_clean ## # A tibble: 6,671 x 5 ## van_id date vol_yes turf_code week ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2019-03-15 1 B 11 ## 2 2 2019-01-30 1 E 5 ## 3 3 2019-02-27 0 E 9 ## 4 4 2019-02-21 0 B 8 ## 5 5 2019-01-13 0 C 2 ## 6 6 2019-01-29 0 E 5 ## 7 7 2019-01-07 0 B 1 ## 8 8 2019-02-15 0 C 7 ## 9 9 2019-01-30 0 D 5 ## 10 10 2019-03-06 0 A 10 ## # … with 6,661 more rows We can use count() again to explore the pledge cards by region and week. We can add unquoted column names as arguments to count() which will be used to group the data. count(canvass_clean, turf_code) ## # A tibble: 6 x 2 ## turf_code n ## &lt;chr&gt; &lt;int&gt; ## 1 A 1137 ## 2 B 1101 ## 3 C 1159 ## 4 D 1114 ## 5 E 1044 ## 6 F 1116 count(canvass_clean, week) ## # A tibble: 11 x 2 ## week n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 651 ## 2 2 629 ## 3 3 552 ## 4 4 637 ## 5 5 607 ## 6 6 614 ## 7 7 617 ## 8 8 643 ## 9 9 641 ## 10 10 640 ## 11 11 440 count(canvass_clean, turf_code, week) ## # A tibble: 66 x 3 ## turf_code week n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 1 114 ## 2 A 2 109 ## 3 A 3 96 ## 4 A 4 121 ## 5 A 5 95 ## 6 A 6 99 ## 7 A 7 78 ## 8 A 8 111 ## 9 A 9 119 ## 10 A 10 119 ## # … with 56 more rows Though these counts (you may be more familiar with the phrase cross-tabs) are extremely useful, we still want to know the number of volunteers pledged. For more control over the aggregate measures, we will use dplyr::group_by() and dplyr::summarise() (for more see chapter 5.6 in R for Data Science). We will create a new table called weekly_canvass which is grouped by turf code and week. This table will have a column for turf_code, week, the number of people pledged to vote n_pledged, and the number of people who indicated they would volunteer vol_yes. weekly_canvass &lt;- canvass_clean %&gt;% group_by(turf_code, week) %&gt;% summarise(n_pledged = n(), vol_yes = sum(vol_yes)) weekly_canvass ## # A tibble: 66 x 4 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 1 114 41 ## 2 A 2 109 28 ## 3 A 3 96 33 ## 4 A 4 121 34 ## 5 A 5 95 32 ## 6 A 6 99 29 ## 7 A 7 78 28 ## 8 A 8 111 29 ## 9 A 9 119 34 ## 10 A 10 119 27 ## # … with 56 more rows Now that we have our counts of pledges and volunteers by week and turf code we need to compare this to their weekly goal. The weekly goals are in goals.csv. goals &lt;- read_csv(&quot;data/goals.csv&quot;) ## Parsed with column specification: ## cols( ## week = col_double(), ## region = col_character(), ## goal = col_double() ## ) goals ## # A tibble: 312 x 3 ## week region goal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A 120 ## 2 1 B 130 ## 3 1 C 70 ## 4 1 D 120 ## 5 1 E 90 ## 6 1 F 130 ## 7 2 A 110 ## 8 2 B 90 ## 9 2 C 130 ## 10 2 D 70 ## # … with 302 more rows Again, this data will need to be joined. What is unique here though is that there is not a single common identifier column. We will need to join on two columns. Namely, region (turf code), and week. Notice that we have mismatched names. To perform a join in this scenario we will need to provide a named vector to the by argument (more on named vectors in chapter 20.4.4 in R for Data Science). The name of the vector element is the column name in the left hand table and the value is the name of the column in the right hand table. In our case, the left hand table is weekly_canvass which has the column name turf_code. The right hand table is goals which has the column name region. To match on this we have to provide the named vector c(&quot;turf_code&quot; = &quot;region). Since the second column we are matching on is week which is present in both tables, this element does not have to be named. Thus the vector we will use is c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;). left_join(weekly_canvass, goals, by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) ## # A tibble: 66 x 5 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes goal ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 120 ## 2 A 2 109 28 110 ## 3 A 3 96 33 60 ## 4 A 4 121 34 60 ## 5 A 5 95 32 90 ## 6 A 6 99 29 120 ## 7 A 7 78 28 110 ## 8 A 8 111 29 130 ## 9 A 9 119 34 130 ## 10 A 10 119 27 130 ## # … with 56 more rows With this join we see that we have the goal and the actual number pledged. We’re one step away from calculating the PTG! To calculate the percent we need to divide the actual number by the goal and multiply by 100. We will do this within a mutate call after we join and save this to a new object ptg. ptg &lt;- left_join(weekly_canvass, goals, by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) %&gt;% mutate(ptg = (n_pledged / goal) * 100) ptg ## # A tibble: 66 x 6 ## # Groups: turf_code [6] ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 120 95 ## 2 A 2 109 28 110 99.1 ## 3 A 3 96 33 60 160 ## 4 A 4 121 34 60 202. ## 5 A 5 95 32 90 106. ## 6 A 6 99 29 120 82.5 ## 7 A 7 78 28 110 70.9 ## 8 A 8 111 29 130 85.4 ## 9 A 9 119 34 130 91.5 ## 10 A 10 119 27 130 91.5 ## # … with 56 more rows ptg %&gt;% ggplot(aes(week, ptg, color = turf_code)) + geom_point() + geom_line() + theme_minimal() + facet_wrap(~turf_code) + geom_hline(yintercept = 100, lty = 2) + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), legend.position = &quot;bottom&quot; ) + labs(title = &quot;PTG by Turf Code&quot;, x = &quot;Week&quot;, y = &quot;%&quot;) While having a table and chart for weekly PTG is extremely useful, it is important to provide a PTG metric for the entire program. To do this we will need to calculate an aggregate measure from the goals table and join this to an aggregated canvass table. We will start by aggregating the goals and creating a total_goals object. Note that in the code chunk below I rename the region column in the group_by() statement this will be useful in the future so we can avoid having to use a named vector in our join. total_goals &lt;- goals %&gt;% group_by(turf_code = region) %&gt;% summarise(goal = sum(goal)) total_goals ## # A tibble: 6 x 2 ## turf_code goal ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4390 ## 2 B 4530 ## 3 C 4470 ## 4 D 4350 ## 5 E 4640 ## 6 F 4720 Now that we have total_goals we need to know the total number of pledges each region has gathered. We will use the existing canvass_clean object and count the total number or pledges using count(). total_pledges &lt;- count(canvass_clean, turf_code) total_pledges ## # A tibble: 6 x 2 ## turf_code n ## &lt;chr&gt; &lt;int&gt; ## 1 A 1137 ## 2 B 1101 ## 3 C 1159 ## 4 D 1114 ## 5 E 1044 ## 6 F 1116 Now we can join these two tables together and calculate a program wide PTG. total_ptg &lt;- inner_join(total_pledges, total_goals, by = &quot;turf_code&quot;) %&gt;% mutate(ptg = n / goal * 100) total_ptg ## # A tibble: 6 x 4 ## turf_code n goal ptg ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1137 4390 25.9 ## 2 B 1101 4530 24.3 ## 3 C 1159 4470 25.9 ## 4 D 1114 4350 25.6 ## 5 E 1044 4640 22.5 ## 6 F 1116 4720 23.6 Everyone loves a bar chart to visually understand their data. total_ptg %&gt;% mutate(turf_code = fct_rev(turf_code)) %&gt;% ggplot(aes(turf_code, ptg)) + geom_col() + geom_hline(yintercept = 100, lty = 2) + coord_flip() + labs(title = &quot;Cumulative PTG&quot;) + theme_minimal() + theme( panel.grid.major.x = element_blank() ) To Do: Turn into a report turn into parameterized report emailing with gmailr hosting and scheduling with connect "],
["ptg-sql.html", "Chapter 3 ptg-sql", " Chapter 3 ptg-sql create sqllite db for the existing sample data go over creating the appropriate sql code. "],
["reporting-googlesheets.html", "Chapter 4 reporting-googlesheets", " Chapter 4 reporting-googlesheets Goal: Create a daily percent to goal report As described previously, a common practice is to export aggregated code from Civis into a Google Sheet. In this section we will work with Google Sheets to create a report that can be used to schedule automatic reporting. To use the googlesheets package you must first install it (install.packages(&quot;googlesheets&quot;)). To load a spreadsheet use the gs_title() or gs_url() functions. The former takes the name of the sheet and the latter uses the url of it. Once this line is ran Google will open and require you to authenticate. This will create .httr-oath file in your working directory. This contains your authorization token which will be used later for automating this workflow. library(googlesheets) # register the sheet sheet &lt;- gs_title(&quot;R 4 Progressive Campaigns&quot;) ## Auto-refreshing stale OAuth token. ## Sheet successfully identified: &quot;R 4 Progressive Campaigns&quot; # read the `weekly_canvas` tab weekly_canvass &lt;- gs_read(sheet, &quot;weekly_canvass&quot;) ## Accessing worksheet titled &#39;weekly_canvass&#39;. ## Parsed with column specification: ## cols( ## turf_code = col_character(), ## week = col_double(), ## n_pledged = col_double(), ## vol_yes = col_double() ## ) # read the `goals` tab goals &lt;- gs_read(sheet, &quot;goals&quot;) ## Accessing worksheet titled &#39;goals&#39;. ## Parsed with column specification: ## cols( ## week = col_double(), ## region = col_character(), ## goal = col_double() ## ) weekly_canvass ## # A tibble: 66 x 4 ## turf_code week n_pledged vol_yes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 114 41 ## 2 A 2 109 28 ## 3 A 3 96 33 ## 4 A 4 121 34 ## 5 A 5 95 32 ## 6 A 6 99 29 ## 7 A 7 78 28 ## 8 A 8 111 29 ## 9 A 9 119 34 ## 10 A 10 119 27 ## # … with 56 more rows goals ## # A tibble: 312 x 3 ## week region goal ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A 120 ## 2 1 B 130 ## 3 1 C 70 ## 4 1 D 120 ## 5 1 E 90 ## 6 1 F 130 ## 7 2 A 110 ## 8 2 B 90 ## 9 2 C 130 ## 10 2 D 70 ## # … with 302 more rows Notice that this code creates two tibbles in memory. Now everything else is the same. We will work within the context of an R Markdown document (intro to R Markdown). We want to create a simple report which will be emailed out to the organizing director each morning with the most up to date weekly PTG. Since this is a weekly report, we want to filter to the current week. In this case, the most recent data is from the 11th week of the month. We will filter both the weekly_canvass and goals tibbles to these weeks and then join. Note that it is important to filter before joining as joining can be computationally intensive. We want to reduce the data before joining it. current_week &lt;- weekly_canvass %&gt;% filter(week == 11) %&gt;% left_join(filter(goals, week == 11), by = c(&quot;turf_code&quot; = &quot;region&quot;, &quot;week&quot;)) %&gt;% mutate(ptg = n_pledged / goal) current_week ## # A tibble: 6 x 6 ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 11 76 20 90 0.844 ## 2 B 11 51 20 50 1.02 ## 3 C 11 87 17 130 0.669 ## 4 D 11 83 31 60 1.38 ## 5 E 11 55 20 70 0.786 ## 6 F 11 88 29 70 1.26 While it nice to have each region’s PTG, it’s also useful to have the entire program weekly PTG. We can create an aggregate table and bind it to the existing table. totals &lt;- current_week %&gt;% bind_rows( current_week %&gt;% summarise(n_pledged = sum(n_pledged), goal = sum(goal), ptg = n_pledged / goal, vol_yes = sum(vol_yes), turf_code = &quot;Total&quot;, week = mean(week)) ) totals ## # A tibble: 7 x 6 ## turf_code week n_pledged vol_yes goal ptg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 11 76 20 90 0.844 ## 2 B 11 51 20 50 1.02 ## 3 C 11 87 17 130 0.669 ## 4 D 11 83 31 60 1.38 ## 5 E 11 55 20 70 0.786 ## 6 F 11 88 29 70 1.26 ## 7 Total 11 440 137 470 0.936 totals %&gt;% ggplot(aes(turf_code, ptg)) + geom_col() + geom_hline(yintercept = 1, lty = 2, alpha = .4) + theme_minimal() + scale_y_continuous(labels = scales::percent_format()) + labs(title = &quot;Weekly PTG&quot;, y = &quot;% to goal&quot;, x = &quot;Turf Code&quot;) It is important that a clean table is presented alongside the chart. For this we will use knitr::kable(). totals %&gt;% mutate(ptg = ptg * 100) %&gt;% select(`Turf Code` = turf_code, `# Pledged` = n_pledged, Goal = goal, `% to Goal` = ptg) %&gt;% knitr::kable(digits = 2) Turf Code # Pledged Goal % to Goal A 76 90 84.44 B 51 50 102.00 C 87 130 66.92 D 83 60 138.33 E 55 70 78.57 F 88 70 125.71 Total 440 470 93.62 "],
["reporting.html", "Chapter 5 reporting", " Chapter 5 reporting creating reproducible reports common reports: daily emails for weekly ptg end of the week ptg weekly cumulative report parameterized r markdown hosting on connect "],
["mapping-doors-targets.html", "Chapter 6 mapping-doors-targets", " Chapter 6 mapping-doors-targets create choropleth of canvassing PTG by district "],
["shiny-creating-interactive-dashboards.html", "Chapter 7 shiny - creating interactive dashboards", " Chapter 7 shiny - creating interactive dashboards "],
["texting.html", "Chapter 8 texting", " Chapter 8 texting canvass &lt;- read_csv(&quot;data/canvassing_results.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## date = col_date(format = &quot;&quot;), ## vol_yes = col_double() ## ) van_names &lt;- read_csv(&quot;data/van_names.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## first_name = col_character(), ## last_name = col_character(), ## date_of_birth = col_date(format = &quot;&quot;), ## age = col_double() ## ) turf_lookup &lt;- read_csv(&quot;data/van_turf_lookup.csv&quot;) ## Parsed with column specification: ## cols( ## van_id = col_double(), ## turf_code = col_character() ## ) universe &lt;- inner_join(canvass, van_names) %&gt;% group_by(van_id) %&gt;% # for duplicates grab the one where they indicated vol yes top_n(1, wt = vol_yes) %&gt;% # make sure there is only 1 observation per person in the case that # duplicates had the same vol_yes result sample_n(size = 1) %&gt;% ungroup() %&gt;% #join region for hypothetical polling location / organizer info left_join(turf_lookup) ## Joining, by = &quot;van_id&quot; ## Joining, by = &quot;van_id&quot; We want the message to be coming from the proper regional organizing director (ROD). We will make some fake names for our RODs. We will create a named vector where the name is the turf code and the value is the organizer’s name (sampled from the babynames package / dataset cite here). organizers &lt;- c(&quot;Rosaleen&quot;, &quot;Larissa&quot;, &quot;Lafayette&quot;, &quot;Theo&quot;, &quot;Zamere&quot;, &quot;Colleen&quot;) names(organizers) &lt;- LETTERS[1:6] organizers ## A B C D E F ## &quot;Rosaleen&quot; &quot;Larissa&quot; &quot;Lafayette&quot; &quot;Theo&quot; &quot;Zamere&quot; &quot;Colleen&quot; We will use stringr::str_replace_all() to create an organizer column universe %&gt;% mutate(organizer = str_replace_all(turf_code, organizers)) %&gt;% select(organizer, everything()) ## # A tibble: 6,004 x 9 ## organizer van_id date vol_yes first_name last_name date_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 Larissa 1 2019-01-05 1 Timika Ehrsam 1970-01-03 ## 2 Zamere 2 2019-01-30 1 Johanna Gorden 1973-12-15 ## 3 Zamere 3 2019-02-27 0 Parys Stoelting 1997-02-15 ## 4 Larissa 4 2019-01-06 0 Lavell Dewall 1992-10-23 ## 5 Lafayette 5 2019-03-02 0 Brenisha Pachter 1977-07-05 ## 6 Zamere 6 2019-01-13 1 Hoang Millon 1992-02-29 ## 7 Larissa 7 2019-01-14 1 Rishith Pisciotto 1987-09-25 ## 8 Lafayette 8 2019-01-28 1 Maximilian Arakawa 1999-05-14 ## 9 Theo 9 2019-02-18 0 Timmie Schlag 1965-06-12 ## 10 Rosaleen 10 2019-02-05 0 Swetha Spreitzer 2000-12-08 ## # … with 5,994 more rows, and 2 more variables: age &lt;dbl&gt;, turf_code &lt;chr&gt; Say each region has their own unique polling location (realistically this will be a much more fine grain dataset that you can join on). We can specify the polling locations using a case_when() function call. We will build upon the previous pipe line. In case when you specify a logical statement and then return a value using the ~—i.e. something == TRUE ~ &quot;if true value&quot;. universe_locations &lt;- universe %&gt;% mutate(organizer = str_replace_all(turf_code, organizers), polling_place = case_when( turf_code == &quot;A&quot; ~ &quot;Community Center&quot;, turf_code == &quot;B&quot; ~ &quot;High School&quot;, turf_code == &quot;C&quot; ~ &quot;Town Hall&quot;, turf_code == &quot;D&quot; ~ &quot;Elementary School&quot;, turf_code == &quot;E&quot; ~ &quot;Rotary Club&quot;, turf_code == &quot;F&quot; ~ &quot;Senior Center&quot; ) ) universe_locations ## # A tibble: 6,004 x 10 ## van_id date vol_yes first_name last_name date_of_birth age ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 2019-01-05 1 Timika Ehrsam 1970-01-03 49 ## 2 2 2019-01-30 1 Johanna Gorden 1973-12-15 46 ## 3 3 2019-02-27 0 Parys Stoelting 1997-02-15 22 ## 4 4 2019-01-06 0 Lavell Dewall 1992-10-23 27 ## 5 5 2019-03-02 0 Brenisha Pachter 1977-07-05 42 ## 6 6 2019-01-13 1 Hoang Millon 1992-02-29 27 ## 7 7 2019-01-14 1 Rishith Pisciotto 1987-09-25 32 ## 8 8 2019-01-28 1 Maximilian Arakawa 1999-05-14 20 ## 9 9 2019-02-18 0 Timmie Schlag 1965-06-12 54 ## 10 10 2019-02-05 0 Swetha Spreitzer 2000-12-08 19 ## # … with 5,994 more rows, and 3 more variables: turf_code &lt;chr&gt;, ## # organizer &lt;chr&gt;, polling_place &lt;chr&gt; Generally, it is useful to segment texting scripts to allow for more tailored messaging. It is recommended to treat your potential volunteers differently than those who have not indicated a desire to volunteer. Let’s go ahead and create two different tibbles, one for vol yes and vol no. Based on this, we will create custom scripts. vol_yes &lt;- filter(universe_locations, vol_yes == 1) vol_no &lt;- filter(universe_locations, vol_yes == 0) At this point you should always check to see if your segmentation has missed anyone. The sum of the number of rows in your two tables should add up to the total number of rows in the original tibble (universe_locations). Let’s perform that sanity check before moving on. nrow(vol_yes) + nrow(vol_no) == nrow(universe_locations) ## [1] TRUE This returns TRUE, we are good to move onward! If there were any missing rows, I would recommend finding a way to incorporate them into some generic universe. The next step is to create the script. The package glue allows us to create character strings with the expressions or values from a tibble. Learn more here. vol_yes_message &lt;- vol_yes %&gt;% mutate(message = glue::glue(&quot;Hi {first_name} this is {organizer} with Abraham Lincoln for the Union! The election is right around the corner. We need all the help we can get, can we count on you to volunteer at {polling_place} on election day?&quot;)) vol_no_message &lt;- vol_no %&gt;% mutate(message = glue::glue(&quot;Hi {first_name} this is {organizer} with Abraham Lincoln for the Union! The election is right around the corner. Your polling location is at the {polling_place}. Can we count on your vote?&quot;)) messages &lt;- bind_rows(vol_yes_message, vol_no_message) ## Warning in bind_rows_(x, .id): Vectorizing &#39;glue&#39; elements may not preserve ## their attributes ## Warning in bind_rows_(x, .id): Vectorizing &#39;glue&#39; elements may not preserve ## their attributes select(messages, message) ## # A tibble: 6,004 x 1 ## message ## &lt;chr&gt; ## 1 Hi Timika this is Larissa with Abraham Lincoln for the Union! The elect… ## 2 Hi Johanna this is Zamere with Abraham Lincoln for the Union! The elect… ## 3 Hi Hoang this is Zamere with Abraham Lincoln for the Union! The electio… ## 4 Hi Rishith this is Larissa with Abraham Lincoln for the Union! The elec… ## 5 Hi Maximilian this is Lafayette with Abraham Lincoln for the Union! The… ## 6 Hi Deeksha this is Lafayette with Abraham Lincoln for the Union! The el… ## 7 Hi Therron this is Lafayette with Abraham Lincoln for the Union! The el… ## 8 Hi Lonney this is Lafayette with Abraham Lincoln for the Union! The ele… ## 9 Hi Robby this is Theo with Abraham Lincoln for the Union! The election … ## 10 Hi Siah this is Rosaleen with Abraham Lincoln for the Union! The electi… ## # … with 5,994 more rows Once you have created your custom messaging you can write this to a csv and upload it into a peer to peer texting platform like Relay and Hustle. In there you can, hopefully, map the VAN IDs so that the text messages are recorded in VAN (talk to your VAN Admin about setting up these integrations). One problem that you might face with platforms like Relay and Hustle is that custom fields can have a character limit. There are a few ways to handle this. One way is by recreating the custom messages within the platform themselves. However, I have found this historically somewhat cumbersom. My work around was to split each sentence into it’s own custom field. We can split the message into the sentences. This will create a list column which we will then unnest (working with list columns by Garret Grolemund). final_message &lt;- messages %&gt;% mutate(message = str_split(message, boundary(&quot;sentence&quot;))) %&gt;% unnest() %&gt;% group_by(van_id) %&gt;% mutate(message_number = row_number(), message_number = paste0(&quot;message_&quot;,message_number)) %&gt;% ungroup() %&gt;% spread(message_number, message) select(final_message, contains(&quot;message_&quot;)) ## # A tibble: 6,004 x 4 ## message_1 message_2 message_3 message_4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Hi Timika this is La… &quot;The election i… We need all the help … &lt;NA&gt; ## 2 &quot;Hi Johanna this is Z… &quot;The election i… We need all the help … &lt;NA&gt; ## 3 &quot;Hi Hoang this is Zam… &quot;The election i… We need all the help … &lt;NA&gt; ## 4 &quot;Hi Rishith this is L… &quot;The election i… We need all the help … &lt;NA&gt; ## 5 &quot;Hi Maximilian this i… &quot;The election i… We need all the help … &lt;NA&gt; ## 6 &quot;Hi Deeksha this is L… &quot;The election i… We need all the help … &lt;NA&gt; ## 7 &quot;Hi Therron this is L… &quot;The election i… We need all the help … &lt;NA&gt; ## 8 &quot;Hi Lonney this is La… &quot;The election i… We need all the help … &lt;NA&gt; ## 9 &quot;Hi Robby this is The… &quot;The election i… We need all the help … &lt;NA&gt; ## 10 &quot;Hi Siah this is Rosa… &quot;The election i… We need all the help … &lt;NA&gt; ## # … with 5,994 more rows Final step: upload to relay / hustle / whatever platform you use. Blast ’em. "],
["google-trends-results.html", "Chapter 9 google trends results 9.1 Google Trends Data 9.2 trendyy 9.3 Visualizing Trends", " Chapter 9 google trends results Over the past few years we have seen Google Trends becoming quite ubiquitous in politics. Pundits have used Google seach trends as talking points. It is not uncommon to hear news about a candidates search trends the days following a town hall or significant rally. It seems that Google trends are becoming the go to proxy for a candidate’s salience. As a campaign, you are interested in the popularity of a candidate relative to another one. If candidate A has seen a gain from 50 to 70, that is all well and good. But how does that compare with candidates C and D? There are others potential use cases—that may be less fraught with media interruptions. For example, one can keep track of the popularity of possible policy issues—i.e. healthcare, gun safety, women’s rights. Though the usefulness of Google Trends search popularity is still unclear, it may be something that your campaign might like to track. In this chapter we will explore how to acquire and utilize trend data using R. This chapter will describe how one can utilize Google Trends data to compare candidate search popularity and view related search terms. This will be done with the tidyverse, and the package trendyy for accessing this data. 9.1 Google Trends Data 9.1.1 Relative Popularity The key metric that Google Trends provides is the relative popularity of a search term by a given geography. Relative search popularity is scaled from 0 to 100. This number is scaled based on population and geography size (for more information go here). This number may be useful on it’s own, but the strength of Google Trends is it’s ability to compare multiple terms. Using Google Trends we can compare up to 5 search terms—presumably candidates. 9.1.2 Related Queries In addition to popularity, Google Trends provides you with related queries. This can help your media team understand in what context their candidate is being associated online. 9.2 trendyy Now that we have an intuition of how Google Trends may be utilized, we will look at how actually acquire these data in R. To get started install the package using install.packages(&quot;trendyy&quot;). Once the package is installed, load the tidyverse and trendyy. library(trendyy) library(tidyverse) In this example we will look at the top five polling candidates as of today (6/10/2019). These are, in no particular order, Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren. Create a vector with the search terms that you will use (in this case the above candidates). candidates &lt;- c(&quot;Joe Biden&quot;, &quot;Kamala Harris&quot;, &quot;Beto O&#39;Rourke&quot;, &quot;Bernie Sanders&quot;, &quot;Elizabeth Warren&quot;) Next we will use the trendyy package to get search popularity. The function trendy() has three main arguments: search_terms, from, and to (in the form of &quot;yyyy-mm-dd&quot;). The first argument is the only mandatory one. Provide a vector of length 5 or less as the first argument. Here we will use the candidates vector and look at data from the past two weeks. I will create two variables for the beginning and end dates. This will be to demonstrate how functions can be used to programatically search date ranges. # to today end &lt;- Sys.Date() # from 2 weeks ago begin &lt;- Sys.Date() - 14 Pass these arguments to trendy() and save them to a variable. candidate_trends &lt;- trendy(search_terms = candidates, from = begin, to = end) candidate_trends ## ~Trendy results~ ## ## Search Terms: Joe Biden, Kamala Harris, Beto O&#39;Rourke, Bernie Sanders, Elizabeth Warren ## ## (&gt;^.^)&gt; ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ &lt;(^.^&lt;) ## # A tibble: 5 x 5 ## keyword max_hits min_hits from to ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 Bernie Sanders 48 31 2019-06-12 2019-06-23 ## 2 Beto O&#39;Rourke 1 1 2019-06-12 2019-06-23 ## 3 Elizabeth Warren 43 29 2019-06-12 2019-06-23 ## 4 Joe Biden 100 35 2019-06-12 2019-06-23 ## 5 Kamala Harris 25 11 2019-06-12 2019-06-23 Trendy creates an object of class trendy see class(candidate_trends) trendy. There are a number of accessor functions. We will use get_interest() and get_related_queries(). See the documentation of the others. To access to relative popularity, we will use get_interest(trendy). popularity &lt;- get_interest(candidate_trends) popularity ## # A tibble: 60 x 7 ## date hits geo time keyword gprop category ## &lt;dttm&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-06-12 00:00:00 92 world 2019-06-12 2… Joe Bid… web All catego… ## 2 2019-06-13 00:00:00 61 world 2019-06-12 2… Joe Bid… web All catego… ## 3 2019-06-14 00:00:00 46 world 2019-06-12 2… Joe Bid… web All catego… ## 4 2019-06-15 00:00:00 35 world 2019-06-12 2… Joe Bid… web All catego… ## 5 2019-06-16 00:00:00 38 world 2019-06-12 2… Joe Bid… web All catego… ## 6 2019-06-17 00:00:00 43 world 2019-06-12 2… Joe Bid… web All catego… ## 7 2019-06-18 00:00:00 47 world 2019-06-12 2… Joe Bid… web All catego… ## 8 2019-06-19 00:00:00 73 world 2019-06-12 2… Joe Bid… web All catego… ## 9 2019-06-20 00:00:00 100 world 2019-06-12 2… Joe Bid… web All catego… ## 10 2019-06-21 00:00:00 69 world 2019-06-12 2… Joe Bid… web All catego… ## # … with 50 more rows For related queries we will use get_related_queries(trendy). Note that you can either pipe the object or pass it directly. candidate_trends %&gt;% get_related_queries() %&gt;% # picking queries for a random candidate filter(keyword == sample(candidates, 1)) ## # A tibble: 0 x 5 ## # … with 5 variables: subject &lt;chr&gt;, related_queries &lt;chr&gt;, value &lt;chr&gt;, ## # keyword &lt;chr&gt;, category &lt;chr&gt; 9.3 Visualizing Trends I’m guessing your director enjoys charts—so do I. To make the data more accessible, use the popularity tibble to create a time series chart of popularity over the past two weeks. We will use ggplot2. Remember that time should be displayed on the x axis. We want to have a line for each candidate, so map the color aesthetic to the keyword. ggplot(popularity, aes(x = date, y = hits, color = keyword)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Search Popularity&quot;, title = &quot;Google popularity of top 5 polling candidates&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) "],
["workflow.html", "Chapter 10 workflow", " Chapter 10 workflow This will be an overview of common data infrastructure in progressive campaigns. We need to start with the golden rule of data “if it is not in VAN, it does not exist”. What is VAN? Voter Activation Network It is not the VAN it is VAN. go over types of voter outreach VAN is the location where all voter interactions live. VAN is our database for voter outreach. When contact is made with a potential voter that is entered in VAN. There are many common ways that this data is recorded. Crowd canvassing—collecting pledge cards, petitioon sign ups. Door-to-door canvassing—knocking on doors and having direct conversations with voters. This is sometimes referred to “knocking doors”. Two other common types of voter outreach is phone banking and text banking. Data is collected and entered into VAN - pledge cards are collected, then entered into VAN (the same night) What is Civis? Civis analytis is a company that creates a data sciecne platforms which is used by many progressive campaigns. They provide secure cloud based solution for data warehousing, analysis, and workflow automation, among others. It is common for many campaigns to have daily exports from VAN into Civis. This provides data directors and data managers the ability to programmatically analyse VAN data. Since data are avaialble in a database, they can be queried using SQL. SQL scripts are written to create aaggregated data. A lot of the power of this comes from the ability to create workflows in Civis. The output of SQL scripts can be exported to a Google Sheet which is much more accessible for the less technically inclined. This also is great for creating shareable sheets-based reporting. The workflow: This guide/book will try to replicate this workflow as best as possible. "],
["web-scraping-polling-use-case.html", "Chapter 11 Web-Scraping: Polling use case 11.1 Understanding rvest 11.2 Example 11.3 Creating historic polling data", " Chapter 11 Web-Scraping: Polling use case A very important metric to keep track of is how your candidate is polling. Are they gaining a lead in the polls or falling behind? This data is often reported via traditional news organizations or some other mediums. The supposed demi-God and mythical pollster Nate Silver’s organization FiveThirtyEight does a wonderful job aggregating polls. Their page National 2020 Democratic Presidential Primary Polls has a table of the most recent polls from many different pollsters. In this use case we will acquire this data by web scraping using rvest. We will also go over ways to programatically save polls results to a text file. Saving polling results can allow you present a long term view of your candidate’s growth during the quarter. 11.1 Understanding rvest This use case will provide a cursory overview of the package rvest. To learn more go here. Web scraping is the process of extracting data from a website. Websites are written in HTML and CSS. There are a few aspects of these languages that are used in web scraping that is important to know. HTML is written in a series of what are call tags. A tag is a set of characters wrapped in angle brackets—i.e. &lt;img&gt;. With CSS (cascading style sheets), web developers can give unique identifiers to a tag. Classes can also be assigned to a tag. Think of these as group. With web scraping we can specify a particular part of a website by it’s HTML tag and perhaps it’s class or ID. rvest provides a large set of functions to make this simpler. 11.2 Example For this example we will be scraping FiveThirtyEight’s aggregated poll table. The table can be found at https://projects.fivethirtyeight.com/2020-primaries/democratic/national/. Before we begin, we must always prepare our workspace. Mise en place. library(rvest) ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) The first thing we will have to do is specify what page we will be scraping from. html_session() will simulate a session in an html browser. By providing a URL to html_session() we will then be able to access the underlying code of that page. Create an object called session by providing the FiveThirtyEight URL to html_session(). session &lt;- html_session(&quot;https://projects.fivethirtyeight.com/2020-primaries/democratic/national/&quot;) The next and most important step is to identify which piece of HTML code contains the table. The easiest way to do this is to open up the webpage in Chrome and open up the Inspect Elements view (on Mac - ⌘ + Shift + C). Now that this is open, click the select element button at the top left corner of the inspection pane. Now hover over the table. You will see that the HTML element is highlighted. We can see that it is a table tag. Additionally we see that there are two different classes polls-table and tracker. To specify a class we put a preceding . to the class name—i.e. .class-name. If there are multiple classes we just append the second class name to it—i.e. .first-class.second-class. Be aware that these selectors can be quite finicky and be a bit difficult to figure out. You might need to do some googling or playing around with the selector. To actually access the content of this HTML element, we must specify the element using the proper selector. html_node() will be used to do this. Provide the html session and the CSS selector to html_node() to extract the HTML element. session %&gt;% html_node(&quot;.polls-table.tracker&quot;) ## {xml_node} ## &lt;table class=&quot;polls-table tracker&quot;&gt; ## [1] &lt;thead class=&quot;hide-mobile&quot; id=&quot;table-header&quot;&gt;&lt;tr&gt;\\n&lt;th class=&quot;new&quot;&gt;&lt; ... ## [2] &lt;tbody&gt;\\n&lt;tr class=&quot;visible-row&quot; data-id=&quot;98292&quot;&gt;\\n&lt;!-- Shared--&gt;&lt;td ... Here we see that this returns on object of class xml_node. This object returns some HTML code but it is still not entirely workable. Since this is an HTML table we want to extract we can use the handy html_table(). Note that if this wasn’t a table but rather text, you can use html_text(). session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table() Take note of the extremely informative error. It appears we might have to deal with mismatching columns. session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table(fill = TRUE) ## Dates Pollster Sample ## 1 • Jun 21-24, 2019457 LV Jun 21-24, 2019 B+Emerson College ## 2 • Jun 17-23, 201916,188 LV Jun 17-23, 2019 B-Morning Consult ## 3 • Jun 16-18, 2019576 LV Jun 16-18, 2019 BYouGov ## 4 • Jun 12-17, 2019306 RV Jun 12-17, 2019 A+Monmouth University ## 5 • Jun 10-16, 201917,226 LV Jun 10-16, 2019 B-Morning Consult ## 6 • Jun 14-15, 2019424 RV Jun 14-15, 2019 C+HarrisX ## 7 • Jun 11-15, 2019385 LV Jun 11-15, 2019 B+Suffolk University ## 8 • Jun 10-13, 20191,000 LV Jun 10-13, 2019 BWPA Intelligence* ## 9 • Jun 9-12, 2019449 LV Jun 9-12, 2019 AFox News ## 10 • Jun 9-11, 2019513 LV Jun 9-11, 2019 BYouGov ## 11 • Jun 6-10, 2019503 RV Jun 6-10, 2019 A-Quinnipiac University ## 12 • Jun 5-10, 20191,621 LV Jun 5-10, 2019 C+Change Research ## 13 • Jun 3-9, 201917,012 LV Jun 3-9, 2019 B-Morning Consult ## 14 • May 29-Jun 5, 20192,271 RV May 29-Jun 5, 2019 B+Ipsos ## 15 • May 29-Jun 5, 20192,525 A May 29-Jun 5, 2019 B+Ipsos ## 16 • Jun 2-4, 2019550 LV Jun 2-4, 2019 BYouGov ## Sample Biden Sanders Warren Harris O&#39;Rourke Buttigieg Booker Klobuchar ## 1 457 LV 34% 27% 14% 7% 1% 6% 3% ## 2 16,188 LV 38% 19% 13% 6% 4% 7% 3% ## 3 576 LV 26% 13% 14% 7% 4% 9% 2% ## 4 306 RV 32% 14% 15% 8% 3% 5% 2% ## 5 17,226 LV 38% 19% 11% 7% 4% 7% 3% ## 6 424 RV 35% 13% 7% 5% 6% 4% 3% ## 7 385 LV 30% 15% 10% 8% 2% 9% 2% ## 8 1,000 LV 35% 14% 10% 9% 4% 8% 3% ## 9 449 LV 32% 13% 9% 8% 4% 8% 3% ## 10 513 LV 26% 12% 16% 6% 3% 8% 2% ## 11 503 RV 30% 19% 15% 7% 3% 8% 1% ## 12 1,621 LV 26% 21% 19% 8% 3% 14% 1% ## 13 17,012 LV 37% 19% 11% 7% 4% 7% 3% ## 14 2,271 RV 31% 14% 9% 6% 3% 5% 2% ## 15 2,525 A 30% 15% 8% 6% 4% 5% 2% ## 16 550 LV 27% 16% 11% 8% 2% 9% 2% ## Castro Yang Gillibrand Gabbard Hickenlooper Delaney Ryan Inslee Bullock ## 1 1% 0% 1% 1% 0% 0% 0% 0% 1% ## 2 1% 1% 2% 1% 1% 1% 1% 1% 1% ## 3 1% 1% 1% 1% 1% 0% 1% 0% 0% ## 4 1% 0% 2% 0% 1% 0% 0% 0% 1% ## 5 1% 1% 1% 1% 1% 1% 1% 1% 0% ## 6 1% 2% 0% 0% 0% 0% 1% 1% 0% ## 7 0% 1% 1% 0% 0% 1% 0% 1% 0% ## 8 1% 1% 1% 0% 0% 0% 0% 0% 0% ## 9 2% 1% 2% 1% 1% 0% 1% 1% 0% ## 10 1% 1% 1% 1% 0% 0% 0% 0% 1% ## 11 1% 0% 1% 0% 0% 0% 0% 1% 0% ## 12 1% 0% 1% 1% 1% 0% 0% 1% 0% ## 13 2% 1% 1% 1% 1% 1% 1% 1% 1% ## 14 2% 1% 1% 0% 1% 1% 0% 1% 0% ## 15 1% 0% 1% 0% 1% 1% 0% 1% 0% ## 16 1% 0% 1% 0% 1% 1% 1% 0% 0% ## Bennet de Blasio Williamson Swalwell Gravel Moulton Messam H. Clinton ## 1 0% 0% 0% 0% 0% 1% 0% 0% ## 2 1% 1% 1% 0% 0% 0% ## 3 1% 1% 1% 0% 0% 0% 0% 0% ## 4 0% 0% 1% 1% 0% 0% 0% 0% ## 5 0% 1% 0% 1% 0% 0% ## 6 1% 1% 0% 0% 1% 0% 0% 1% ## 7 1% 0% 0% 0% 0% 0% 0% 0% ## 8 0% 0% 0% 0% 0% 0% 0% 0% ## 9 0% 0% 0% 0% 0% 0% 0% 0% ## 10 0% 1% 1% 0% 1% 0% 0% 0% ## 11 0% 0% 0% 0% 0% 0% 0% 0% ## 12 0% 0% 0% 0% 0% 1% 0% ## 13 1% 1% 0% 0% 0% 0% ## 14 0% 0% 1% 0% 0% 0% 0% 0% ## 15 0% 0% 1% 0% 0% 0% 0% 0% ## 16 1% 0% 2% 0% 0% 0% 0% 0% ## Bloomberg M. Obama Brown Kerry Abrams Holder McAuliffe Winfrey Ojeda ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 1% ## 13 ## 14 ## 15 ## 16 ## Trump Cuomo Avenatti Kennedy Patrick Zuckerberg Pelosi Garcetti Newsom ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## Steyer Schultz Kaine Johnson Kucinich Lee Scott Sinema Warner NA ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## NA ## 1 Biden34%Sanders27%Warren14%Harris7%Buttigieg6%Booker3%Gillibrand1%O&#39;Rourke1%Klobuchar1%Yang1%Inslee1%Gravel1%Delaney0%de Blasio0%Ryan0%Gabbard0%Williamson0%Hickenlooper0%Bennet0%Castro0%Moulton0%Bullock0%Messam0%Swalwell0% ## 2 Biden38%Sanders19%Warren13%Buttigieg7%Harris6%O&#39;Rourke4%Booker3%Yang2%Bennet1%Bullock1%Castro1%de Blasio1%Delaney1%Gabbard1%Gillibrand1%Hickenlooper1%Inslee1%Klobuchar1%Ryan1%Moulton0%Swalwell0%Williamson0% ## 3 Biden26%Warren14%Sanders13%Buttigieg9%Harris7%O&#39;Rourke4%Booker2%Bennet1%Bullock1%Castro1%de Blasio1%Delaney1%Gabbard1%Gillibrand1%Klobuchar1%Yang1%Gravel0%Hickenlooper0%Inslee0%Messam0%Moulton0%Ryan0%Swalwell0%Williamson0% ## 4 Biden32%Warren15%Sanders14%Harris8%Buttigieg5%O&#39;Rourke3%Booker2%Yang2%de Blasio1%Gabbard1%Inslee1%Klobuchar1%Williamson1%Castro0%Gillibrand0%Ryan0%Bennet0%Bullock0%Delaney0%Gravel0%Hickenlooper0%Messam0%Moulton0%Swalwell0% ## 5 Biden38%Sanders19%Warren11%Buttigieg7%Harris7%O&#39;Rourke4%Booker3%Castro1%Delaney1%Gabbard1%Gillibrand1%Hickenlooper1%Klobuchar1%Ryan1%Williamson1%Yang1%Bennet1%Bullock0%de Blasio0%Inslee0%Moulton0%Swalwell0% ## 6 Biden35%Sanders13%Warren7%O&#39;Rourke6%Harris5%Buttigieg4%Booker3%Castro2%Bennet1%Ryan1%Klobuchar1%Swalwell1%Bullock1%Delaney1%Messam1%Yang0%Gabbard0%Moulton0%Gillibrand0%Williamson0%de Blasio0%Hickenlooper0%Inslee0%Gravel0% ## 7 Biden30%Sanders15%Warren10%Buttigieg9%Harris8%O&#39;Rourke2%Booker2%Castro1%Hickenlooper1%Bullock1%Ryan1%Yang1%de Blasio0%Klobuchar0%Moulton0%Williamson0%Bennet0%Delaney0%Gabbard0%Gillibrand0%Gravel0%Inslee0%Messam0%Swalwell0% ## 8 Biden35%Sanders14%Warren10%Harris9%Buttigieg8%O&#39;Rourke4%Booker3%Castro1%Klobuchar1%Yang1%Bennet0%Bullock0%Delaney0%Gabbard0%Gillibrand0%Hickenlooper0%Inslee0%Messam0%Moulton0%Gravel0%Ryan0%Swalwell0%Williamson0%de Blasio0% ## 9 Biden32%Sanders13%Warren9%Buttigieg8%Harris8%O&#39;Rourke4%Booker3%Klobuchar2%Yang2%Castro1%Delaney1%Gabbard1%Gillibrand1%Ryan1%Bennet0%Bullock0%de Blasio0%Hickenlooper0%Inslee0%Gravel0%Messam0%Moulton0%Swalwell0%Williamson0% ## 10 Biden26%Warren16%Sanders12%Buttigieg8%Harris6%O&#39;Rourke3%Booker2%Bennet1%Castro1%de Blasio1%Gillibrand1%Inslee1%Klobuchar1%Swalwell1%Yang1%Bullock0%Delaney0%Gabbard0%Gravel0%Hickenlooper0%Messam0%Moulton0%Ryan0%Williamson0% ## 11 Biden30%Sanders19%Warren15%Buttigieg8%Harris7%O&#39;Rourke3%Booker1%Klobuchar1%Yang1%Ryan1%Gillibrand0%Castro0%Gabbard0%Inslee0%Hickenlooper0%Delaney0%Williamson0%Messam0%Swalwell0%Moulton0%Bennet0%Bullock0%de Blasio0%Gravel0% ## 12 Biden26%Sanders21%Warren19%Buttigieg14%Harris8%O&#39;Rourke3%Yang1%Abrams1%Booker1%Klobuchar1%Gabbard1%Gravel1%Gillibrand1%Ryan1%Bullock0%Castro0%Inslee0%Swalwell0%Hickenlooper0%de Blasio0%Bennet0%Williamson0%Moulton0%Delaney0% ## 13 Biden37%Sanders19%Warren11%Buttigieg7%Harris7%O&#39;Rourke4%Booker3%Klobuchar2%Bennet1%Bullock1%Castro1%Delaney1%Gabbard1%Gillibrand1%Hickenlooper1%Inslee1%Yang1%Ryan1%de Blasio0%Moulton0%Swalwell0%Williamson0% ## 14 Biden31%Sanders14%Warren9%Harris6%Buttigieg5%O&#39;Rourke3%Booker2%Klobuchar2%Castro1%Gabbard1%Hickenlooper1%Yang1%Ryan1%de Blasio1%Gillibrand0%Bullock0%Inslee0%Delaney0%Williamson0%Messam0%Swalwell0%Moulton0%Bennet0%Gravel0% ## 15 Biden30%Sanders15%Warren8%Harris6%Buttigieg5%O&#39;Rourke4%Booker2%Klobuchar1%Gabbard1%Hickenlooper1%Yang1%Ryan1%de Blasio1%Castro0%Gillibrand0%Bullock0%Inslee0%Delaney0%Williamson0%Messam0%Swalwell0%Moulton0%Bennet0%Gravel0% ## 16 Biden27%Sanders16%Warren11%Buttigieg9%Harris8%Booker2%de Blasio2%O&#39;Rourke2%Bullock1%Delaney1%Gabbard1%Hickenlooper1%Klobuchar1%Yang1%Bennet0%Castro0%Gillibrand0%Gravel0%Inslee0%Messam0%Moulton0%Ryan0%Swalwell0%Williamson0% ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 111 rows ] This is much better! But based on visual inspection the column headers are not properly matched. There are a few things that need to be sorted out: there are two date columns, there are commas and percents where numeric columns should be, the column headers are a little messy, and the table isn’t a tibble (this is just personal preference). We will handle the final two issues first as they are easiest to deal with. The function clean_names() from janitor will handle the column headers, and as_tibble() will coerce the data.frame into a proper tibble. Save this semi-clean tibble into an object called polls. polls &lt;- session %&gt;% html_node(&quot;.polls-table.tracker&quot;) %&gt;% html_table(fill = TRUE) %&gt;% janitor::clean_names() %&gt;% as_tibble() polls ## # A tibble: 127 x 59 ## x dates pollster sample sample_2 biden sanders warren harris ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 • Jun … Jun 21-… B+Eme… 457 LV 34% 27% 14% ## 2 • Jun … Jun 17-… B-Mor… 16,188 LV 38% 19% 13% ## 3 • Jun … Jun 16-… BYouG… 576 LV 26% 13% 14% ## 4 • Jun … Jun 12-… A+Mon… 306 RV 32% 14% 15% ## 5 • Jun … Jun 10-… B-Mor… 17,226 LV 38% 19% 11% ## 6 • Jun … Jun 14-… C+Har… 424 RV 35% 13% 7% ## 7 • Jun … Jun 11-… B+Suf… 385 LV 30% 15% 10% ## 8 • Jun … Jun 10-… BWPA … 1,000 LV 35% 14% 10% ## 9 • Jun … Jun 9-1… AFox … 449 LV 32% 13% 9% ## 10 • Jun … Jun 9-1… BYouG… 513 LV 26% 12% 16% ## # … with 117 more rows, and 50 more variables: o_rourke &lt;chr&gt;, ## # buttigieg &lt;chr&gt;, booker &lt;chr&gt;, klobuchar &lt;chr&gt;, castro &lt;chr&gt;, ## # yang &lt;chr&gt;, gillibrand &lt;chr&gt;, gabbard &lt;chr&gt;, hickenlooper &lt;chr&gt;, ## # delaney &lt;chr&gt;, ryan &lt;chr&gt;, inslee &lt;chr&gt;, bullock &lt;chr&gt;, bennet &lt;chr&gt;, ## # de_blasio &lt;chr&gt;, williamson &lt;chr&gt;, swalwell &lt;chr&gt;, gravel &lt;chr&gt;, ## # moulton &lt;chr&gt;, messam &lt;chr&gt;, h_clinton &lt;chr&gt;, bloomberg &lt;chr&gt;, ## # m_obama &lt;chr&gt;, brown &lt;chr&gt;, kerry &lt;chr&gt;, abrams &lt;chr&gt;, holder &lt;chr&gt;, ## # mc_auliffe &lt;chr&gt;, winfrey &lt;chr&gt;, ojeda &lt;chr&gt;, trump &lt;chr&gt;, ## # cuomo &lt;chr&gt;, avenatti &lt;chr&gt;, kennedy &lt;chr&gt;, patrick &lt;chr&gt;, ## # zuckerberg &lt;chr&gt;, pelosi &lt;chr&gt;, garcetti &lt;chr&gt;, newsom &lt;chr&gt;, ## # steyer &lt;chr&gt;, schultz &lt;chr&gt;, kaine &lt;chr&gt;, johnson &lt;chr&gt;, ## # kucinich &lt;chr&gt;, lee &lt;chr&gt;, scott &lt;chr&gt;, sinema &lt;chr&gt;, warner &lt;chr&gt;, ## # na &lt;chr&gt;, na_2 &lt;chr&gt; We want to shift over the column names to the right just once. Unfortunately there is no elegant way to do this (that I am aware of). We can see that the first column is completely useless so that can be removed. Once that column is removed we can reset the names this way they will be well aligned. We will start by creating a vector of the original column names. col_names &lt;- names(polls) col_names ## [1] &quot;x&quot; &quot;dates&quot; &quot;pollster&quot; &quot;sample&quot; ## [5] &quot;sample_2&quot; &quot;biden&quot; &quot;sanders&quot; &quot;warren&quot; ## [9] &quot;harris&quot; &quot;o_rourke&quot; &quot;buttigieg&quot; &quot;booker&quot; ## [13] &quot;klobuchar&quot; &quot;castro&quot; &quot;yang&quot; &quot;gillibrand&quot; ## [17] &quot;gabbard&quot; &quot;hickenlooper&quot; &quot;delaney&quot; &quot;ryan&quot; ## [21] &quot;inslee&quot; &quot;bullock&quot; &quot;bennet&quot; &quot;de_blasio&quot; ## [25] &quot;williamson&quot; &quot;swalwell&quot; &quot;gravel&quot; &quot;moulton&quot; ## [29] &quot;messam&quot; &quot;h_clinton&quot; &quot;bloomberg&quot; &quot;m_obama&quot; ## [33] &quot;brown&quot; &quot;kerry&quot; &quot;abrams&quot; &quot;holder&quot; ## [37] &quot;mc_auliffe&quot; &quot;winfrey&quot; &quot;ojeda&quot; &quot;trump&quot; ## [41] &quot;cuomo&quot; &quot;avenatti&quot; &quot;kennedy&quot; &quot;patrick&quot; ## [45] &quot;zuckerberg&quot; &quot;pelosi&quot; &quot;garcetti&quot; &quot;newsom&quot; ## [49] &quot;steyer&quot; &quot;schultz&quot; &quot;kaine&quot; &quot;johnson&quot; ## [53] &quot;kucinich&quot; &quot;lee&quot; &quot;scott&quot; &quot;sinema&quot; ## [57] &quot;warner&quot; &quot;na&quot; &quot;na_2&quot; Unfortunately this also presents another issue. Once a column is deselected, there will be one more column name than column. So we will need to select all but the last element of the original names. We will create a vector called new_names. # identify the integer number of the last column last_col &lt;- length(col_names) - 1 # create a vector which will be used for the new names new_names &lt;- col_names[1:last_col] Now we can try implementing the hacky solution. Here we will deselect the first column and reset the names using setNames(). Following, we will use the mutate_at() variant to remove the percent sign from every candidate column and coerce them into integer columns. Here we will specify which variables to not mutate at within vars(). polls %&gt;% select(-1) %&gt;% setNames(new_names)%&gt;% select(-1) %&gt;% mutate_at(vars(-c(&quot;dates&quot;, &quot;pollster&quot;, &quot;sample&quot;, &quot;sample_2&quot;)), ~as.integer(str_remove(., &quot;%&quot;))) ## Warning in (structure(function (..., .x = ..1, .y = ..2, . = ..1) : NAs ## introduced by coercion ## # A tibble: 127 x 57 ## dates pollster sample sample_2 biden sanders warren harris o_rourke ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Jun … B+Emers… 457 LV 34 27 14 7 1 ## 2 Jun … B-Morni… 16,188 LV 38 19 13 6 4 ## 3 Jun … BYouGov 576 LV 26 13 14 7 4 ## 4 Jun … A+Monmo… 306 RV 32 14 15 8 3 ## 5 Jun … B-Morni… 17,226 LV 38 19 11 7 4 ## 6 Jun … C+Harri… 424 RV 35 13 7 5 6 ## 7 Jun … B+Suffo… 385 LV 30 15 10 8 2 ## 8 Jun … BWPA In… 1,000 LV 35 14 10 9 4 ## 9 Jun … AFox Ne… 449 LV 32 13 9 8 4 ## 10 Jun … BYouGov 513 LV 26 12 16 6 3 ## # … with 117 more rows, and 48 more variables: buttigieg &lt;int&gt;, ## # booker &lt;int&gt;, klobuchar &lt;int&gt;, castro &lt;int&gt;, yang &lt;int&gt;, ## # gillibrand &lt;int&gt;, gabbard &lt;int&gt;, hickenlooper &lt;int&gt;, delaney &lt;int&gt;, ## # ryan &lt;int&gt;, inslee &lt;int&gt;, bullock &lt;int&gt;, bennet &lt;int&gt;, ## # de_blasio &lt;int&gt;, williamson &lt;int&gt;, swalwell &lt;int&gt;, gravel &lt;int&gt;, ## # moulton &lt;int&gt;, messam &lt;int&gt;, h_clinton &lt;int&gt;, bloomberg &lt;int&gt;, ## # m_obama &lt;int&gt;, brown &lt;int&gt;, kerry &lt;int&gt;, abrams &lt;int&gt;, holder &lt;int&gt;, ## # mc_auliffe &lt;int&gt;, winfrey &lt;int&gt;, ojeda &lt;int&gt;, trump &lt;int&gt;, ## # cuomo &lt;int&gt;, avenatti &lt;int&gt;, kennedy &lt;int&gt;, patrick &lt;int&gt;, ## # zuckerberg &lt;int&gt;, pelosi &lt;int&gt;, garcetti &lt;int&gt;, newsom &lt;int&gt;, ## # steyer &lt;int&gt;, schultz &lt;int&gt;, kaine &lt;int&gt;, johnson &lt;int&gt;, ## # kucinich &lt;int&gt;, lee &lt;int&gt;, scott &lt;int&gt;, sinema &lt;int&gt;, warner &lt;int&gt;, ## # na &lt;int&gt; Now we must tidy the data. We will use tidyr::gather() to transform the data from wide to long. In short, gather takes the column headers (the key argument) and creates a new variable from the values of the columns (the value argument). In this case, we will create a new column called candidate from the column headers and a second column called points which are a candidates polling percentage. Next we deselect any columns that we do not want to be gathered. polls %&gt;% select(-1) %&gt;% setNames(new_names)%&gt;% select(-1) %&gt;% mutate_at(vars(-c(&quot;dates&quot;, &quot;pollster&quot;, &quot;sample&quot;, &quot;sample_2&quot;)), ~as.integer(str_remove(., &quot;%&quot;))) %&gt;% gather(candidate, points, -dates, -pollster, -sample, -sample_2) ## Warning in (structure(function (..., .x = ..1, .y = ..2, . = ..1) : NAs ## introduced by coercion ## # A tibble: 6,731 x 6 ## dates pollster sample sample_2 candidate points ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jun 21-24, 2019 B+Emerson College 457 LV biden 34 ## 2 Jun 17-23, 2019 B-Morning Consult 16,188 LV biden 38 ## 3 Jun 16-18, 2019 BYouGov 576 LV biden 26 ## 4 Jun 12-17, 2019 A+Monmouth University 306 RV biden 32 ## 5 Jun 10-16, 2019 B-Morning Consult 17,226 LV biden 38 ## 6 Jun 14-15, 2019 C+HarrisX 424 RV biden 35 ## 7 Jun 11-15, 2019 B+Suffolk University 385 LV biden 30 ## 8 Jun 10-13, 2019 BWPA Intelligence* 1,000 LV biden 35 ## 9 Jun 9-12, 2019 AFox News 449 LV biden 32 ## 10 Jun 9-11, 2019 BYouGov 513 LV biden 26 ## # … with 6,721 more rows There are a few more house-keeping things that need to be done to improve this data set. sample_2 is rather uninformative. On the FiveThirtyEight website there is a key which describes what these values represent (A = ADULTS, RV = REGISTERED VOTERS, V = VOTERS, LV = LIKELY VOTERS). This should be specified in our data set. In addition the sample column ought to be cast into an integer column. And finally, those messy dates will need to be cleaned. My approach to this requires creating a function to handle this cleaning. First, the simple stuff. To do the first two above steps, we will continue our function chain and save it to a new variable polls_tidy. polls_tidy &lt;- polls %&gt;% select(-1) %&gt;% setNames(new_names)%&gt;% select(-1) %&gt;% mutate_at(vars(-c(&quot;dates&quot;, &quot;pollster&quot;, &quot;sample&quot;, &quot;sample_2&quot;)), ~as.integer(str_remove(., &quot;%&quot;))) %&gt;% gather(candidate, points, -dates, -pollster, -sample, -sample_2) %&gt;% mutate(sample_2 = case_when( sample_2 == &quot;RV&quot; ~ &quot;Registered Voters&quot;, sample_2 == &quot;LV&quot; ~ &quot;Likely Voters&quot;, sample_2 == &quot;A&quot; ~ &quot;Adults&quot;, sample_2 == &quot;V&quot; ~ &quot;Voters&quot; ), sample = as.integer(str_remove(sample, &quot;,&quot;))) ## Warning in (structure(function (..., .x = ..1, .y = ..2, . = ..1) : NAs ## introduced by coercion polls_tidy ## # A tibble: 6,731 x 6 ## dates pollster sample sample_2 candidate points ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jun 21-24, 2… B+Emerson College 457 Likely Voters biden 34 ## 2 Jun 17-23, 2… B-Morning Consult 16188 Likely Voters biden 38 ## 3 Jun 16-18, 2… BYouGov 576 Likely Voters biden 26 ## 4 Jun 12-17, 2… A+Monmouth Univer… 306 Registered Vot… biden 32 ## 5 Jun 10-16, 2… B-Morning Consult 17226 Likely Voters biden 38 ## 6 Jun 14-15, 2… C+HarrisX 424 Registered Vot… biden 35 ## 7 Jun 11-15, 2… B+Suffolk Univers… 385 Likely Voters biden 30 ## 8 Jun 10-13, 2… BWPA Intelligence* 1000 Likely Voters biden 35 ## 9 Jun 9-12, 20… AFox News 449 Likely Voters biden 32 ## 10 Jun 9-11, 20… BYouGov 513 Likely Voters biden 26 ## # … with 6,721 more rows 11.2.1 Date cleaning Next we must work to clean the date field. I find that when working with a messy column, creating a single function which handles the cleaning is one of the most effective approaches. Here we will create a function which takes a value provided from the dates field and return a cleaned date. There are two unique cases I identified. There are poll dates which occurred during a single month, or a poll that spanned two months. The dates are separated by a single hyphen -. If we split the date at - we will either receive two elements with a month indicated or one month with a day and a day number. In the latter case we will have to carry over the month. Then the year can be appended to it and parsed as a date using the lubridate package. For more on lubridate visit here. The function will only return one date at a time. The two arguments will be date and .return to indicate whether the first or second date should be provided. The internals of this function rely heavily on the stringr package (see R for Data Science Chapter 14). switch() at the end of the function determines which date should be returned (see Advanced R Chapter 5). clean_date &lt;- function(date, .return = &quot;first&quot;) { # take date and split at the comma to get the year and the month-day combo date_split &lt;- str_split(date, &quot;,&quot;) %&gt;% # remove from list / coerce to vector unlist() %&gt;% # remove extra white space str_trim() # extract the year date_year &lt;- date_split[2] # split the month day portion and coerce to vector dates &lt;- unlist(str_split(date_split[1], &quot;-&quot;)) # paste the month day and year together then parse as date using `mdy()` first_date &lt;- paste(dates[1], date_year) %&gt;% lubridate::mdy() second_date &lt;- ifelse(!str_detect(dates[2], &quot;[A-z]+&quot;), yes = paste(str_extract(dates[1], &quot;[A-z]+&quot;), dates[2], date_year), no = paste(dates[2], date_year)) %&gt;% lubridate::mdy() switch(.return, first = return(first_date), second = return(second_date) ) } # test on a date clean_date(polls_tidy$dates[10], .return = &quot;first&quot;) ## [1] &quot;2019-06-09&quot; clean_date(polls_tidy$dates[10], .return = &quot;second&quot;) ## [1] &quot;2019-06-11&quot; We can use this new function to create two new columns poll_start and poll_end using mutate(). Following this we can deselect the original dates column, remove any observations missing a points value, remove duplicates using distinct(), and save this to polls_clean. polls_clean &lt;- polls_tidy %&gt;% mutate(poll_start = clean_date(dates, &quot;first&quot;), poll_end = clean_date(dates, &quot;second&quot;)) %&gt;% select(-dates) %&gt;% filter(!is.na(points)) %&gt;% distinct() polls_clean ## # A tibble: 2,197 x 7 ## pollster sample sample_2 candidate points poll_start poll_end ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; ## 1 B+Emerson Col… 457 Likely Vot… biden 34 2019-06-21 2019-06-24 ## 2 B-Morning Con… 16188 Likely Vot… biden 38 2019-06-21 2019-06-24 ## 3 BYouGov 576 Likely Vot… biden 26 2019-06-21 2019-06-24 ## 4 A+Monmouth Un… 306 Registered… biden 32 2019-06-21 2019-06-24 ## 5 B-Morning Con… 17226 Likely Vot… biden 38 2019-06-21 2019-06-24 ## 6 C+HarrisX 424 Registered… biden 35 2019-06-21 2019-06-24 ## 7 B+Suffolk Uni… 385 Likely Vot… biden 30 2019-06-21 2019-06-24 ## 8 BWPA Intellig… 1000 Likely Vot… biden 35 2019-06-21 2019-06-24 ## 9 AFox News 449 Likely Vot… biden 32 2019-06-21 2019-06-24 ## 10 BYouGov 513 Likely Vot… biden 26 2019-06-21 2019-06-24 ## # … with 2,187 more rows 11.2.2 Visualization The cleaned data can be aggregated and visualized. avg_polls &lt;- polls_clean %&gt;% group_by(candidate) %&gt;% summarise(avg_points = mean(points, na.rm = TRUE), min_points = min(points, na.rm = TRUE), max_points = max(points, na.rm = TRUE), n_polls = n() - sum(is.na(points))) %&gt;% # identify how many polls candidate is in # remove candidates who appear in 50 or fewer polls: i.e. HRC filter(n_polls &gt; 50) %&gt;% arrange(-avg_points) avg_polls ## # A tibble: 18 x 5 ## candidate avg_points min_points max_points n_polls ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 biden 32.1 9 66 119 ## 2 sanders 19.1 4 42 121 ## 3 harris 8.51 2 38 121 ## 4 warren 8.49 2 43 120 ## 5 o_rourke 6.22 1 21 116 ## 6 buttigieg 5.25 0 21 91 ## 7 booker 3.23 0 9 115 ## 8 klobuchar 1.69 0 5 104 ## 9 yang 1.14 0 3 72 ## 10 castro 1.08 0 12 107 ## 11 gillibrand 0.907 0 9 108 ## 12 ryan 0.706 0 2 51 ## 13 gabbard 0.699 0 3 93 ## 14 hickenlooper 0.670 0 2 91 ## 15 delaney 0.516 0 8 93 ## 16 bullock 0.477 0 1 65 ## 17 inslee 0.425 0 2 87 ## 18 williamson 0.241 0 1 54 avg_polls %&gt;% mutate(candidate = fct_reorder(candidate, avg_points)) %&gt;% ggplot(aes(candidate, avg_points)) + geom_col() + theme_minimal() + coord_flip() + labs(title = &quot;Polls Standings&quot;, x = &quot;&quot;, y = &quot;%&quot;) 11.3 Creating historic polling data It may become useful to have a running history of how candidates have been polling. We can use R to write a csv file of the data from FiveThirtyEight. However, what happens when the polls update? How we can we keep the previous data and the new data? We will work through an example using a combination of bind_rows() and distinct(). I want to emphasize that this is not a good practice if you need to scale to hundreds of thousand of rows. This works in this case as the data are inherently small. To start, I have created a sample dataset which contains 80% of these polls (maybe less by the time you do this!). Note that is probably best to version control this or have multiple copies as a failsafe. The approach we will take is to read in the historic polls data set and bind rows with the polls_clean data we have scraped. Next we remove duplicate rows using distinct(). old_polls &lt;- read_csv(&quot;data/polls.csv&quot;) ## Parsed with column specification: ## cols( ## pollster = col_character(), ## sample = col_double(), ## sample_2 = col_character(), ## candidate = col_character(), ## points = col_double(), ## poll_start = col_date(format = &quot;&quot;), ## poll_end = col_date(format = &quot;&quot;) ## ) old_polls ## # A tibble: 1,564 x 7 ## pollster sample sample_2 candidate points poll_start poll_end ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 C+HarrisX 370 Registered… klobuchar 2 2019-06-06 2019-06-10 ## 2 C+HarrisX 448 Registered… gillibra… 1 2019-06-06 2019-06-10 ## 3 B-Morning Con… 11627 Likely Vot… harris 13 2019-06-06 2019-06-10 ## 4 B-Morning Con… 699 Registered… delaney 0 2019-06-06 2019-06-10 ## 5 C+HarrisX 743 Registered… williams… 1 2019-06-06 2019-06-10 ## 6 A-Quinnipiac … 559 Registered… gabbard 0 2019-06-06 2019-06-10 ## 7 B-Morning Con… 14250 Likely Vot… gillibra… 2 2019-06-06 2019-06-10 ## 8 A-Quinnipiac … 559 Registered… gillibra… 0 2019-06-06 2019-06-10 ## 9 B-Morning Con… 14250 Likely Vot… harris 6 2019-06-06 2019-06-10 ## 10 A+Monmouth Un… 330 Registered… warren 8 2019-06-06 2019-06-10 ## # … with 1,554 more rows updated_polls &lt;- bind_rows(old_polls, polls_clean) %&gt;% distinct() updated_polls ## # A tibble: 3,761 x 7 ## pollster sample sample_2 candidate points poll_start poll_end ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 C+HarrisX 370 Registered… klobuchar 2 2019-06-06 2019-06-10 ## 2 C+HarrisX 448 Registered… gillibra… 1 2019-06-06 2019-06-10 ## 3 B-Morning Con… 11627 Likely Vot… harris 13 2019-06-06 2019-06-10 ## 4 B-Morning Con… 699 Registered… delaney 0 2019-06-06 2019-06-10 ## 5 C+HarrisX 743 Registered… williams… 1 2019-06-06 2019-06-10 ## 6 A-Quinnipiac … 559 Registered… gabbard 0 2019-06-06 2019-06-10 ## 7 B-Morning Con… 14250 Likely Vot… gillibra… 2 2019-06-06 2019-06-10 ## 8 A-Quinnipiac … 559 Registered… gillibra… 0 2019-06-06 2019-06-10 ## 9 B-Morning Con… 14250 Likely Vot… harris 6 2019-06-06 2019-06-10 ## 10 A+Monmouth Un… 330 Registered… warren 8 2019-06-06 2019-06-10 ## # … with 3,751 more rows Now you have a cleaned data set which has been integrated with the recently scraped data. Write this to a csv using write_csv() for later use. "]
]
